{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ZhmA2XxJ3oZC7A-U2mpUdB2eZZLz5NfW","timestamp":1665630828505}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["### Final setting\n","After trying the different settings of model hyperparameters and model structure, including the number of ResBlocks and the number of dimension reduction layers (linear layers) We find that the best setting up to now is to have 1 ResBlock and two linear layers before and after the ResBlock. The first intermediate dimension is set to 256. The hidden dimension was set to 3. 3 was chosen as it is the middle between two and four. 2 might be too small and the representation it takes might not be that valid analytically. In addition, 2 is not that well-performing compared with 3 and 4. In contrast, 4 is well-performing. And of course, the more dimensions we have, it might be therefore closer to what we have in brain because we are not sure how many dimensions are there in the brain. However, because 4 is not easy to be plotted graphically, if we want to directly plot them without doing any additional PCA transformation, 4 dimensions might not be that's suitable. Therefore, we chose the middle 3, 3 is OK in terms of performance. And also OK for plotting. \n","\n","In addition, we will train the model for around 20 epochs. "],"metadata":{"id":"B-mljeGlqMqo"}},{"cell_type":"markdown","source":["### Core changes done compared with very original model last sem\n","1. Reduced hidden dimension (latent dim) to 3, which is not a large number but still allows considerable dimensionality of phoneme system. \n","2. Reduced 3 ResBlocks to 2 ResBlocks, which alleviates the influence of reducing hidden dimension on HCV scores. The training and validation loss, though, cannot be solved by reducing number of ResBlocks. \n","3. The resulting HCV score was satisfactory, which is not very much affected by the change. "],"metadata":{"id":"UC3er5W14fAz"}},{"cell_type":"markdown","source":["### Task here\n","After many trials, it was found that changing the model structure might not bring advantage. Downsizing the hidden dimensions usually highers loss and lowers HCV scores. But by reducing the number of ResBlocks it can be alleviated. \n","\n","This time I will try to recover to the old model but just 1. reduce hidden dim to 4 or 3 (2 is okay as well, but might be too simple? ), and 2. reduce number of Res block. "],"metadata":{"id":"eyFx-CAItkeh"}},{"cell_type":"markdown","source":["### Core task\n","The core task for this copy is to test models with less parameters and see whether they can still catch the phonetic system. \n","\n","In this round's revision, the most important thing is to verify that the model is really doing something, that what we have is really due to having the model. \n","\n","One thing to note is that we should not deny that the input data themselves are having a phonetic system, it actually must be so, because the model (and even human) is just expecte to learn and extract this knowledge from the naturally existing structure of sounds. So we cannot run a phoneme evaluation on MFCCs (or even raw wav data) and declare that our model is not making things better. No, the original thing is the best, we are trying to verify that after some kind of learning and abstraction, whether the phonetic system can be copied into the knowledge of our learner. "],"metadata":{"id":"wN2xjbOojl_9"}},{"cell_type":"code","source":["# Mount Google Drive: connect to google drive storage\n","# Should be changed to other codes when using HPC or running locally. \n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Specify directory of course materials in Google Drive\n","main_dir = '/content/drive/My Drive/FeatureLearning/'\n","\n","# (Ref.: https://stackoverflow.com/questions/48905127/importing-py-files-in-google-colab)\n","import sys, os\n","sys.path.append(main_dir)\n","# os.listdir(main_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yc8_zIAzXE6n","executionInfo":{"status":"ok","timestamp":1675766041662,"user_tz":-480,"elapsed":20891,"user":{"displayName":"Frank Learning","userId":"10777113402383858671"}},"outputId":"686f68fd-7400-433f-bc67-97e5b16184f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import zipfile\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch import optim\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","import datetime as dt\n","import shutil\n","import numpy as np\n","import csv\n","import pandas as pd\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import homogeneity_completeness_v_measure\n","import pickle"],"metadata":{"id":"jN5DNuExjwet"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#################################################################\n","#             Directory              #\n","#################################################################\n","\n","root_dir = \"/content/\"\n","base_dir = \"/content/drive/My Drive/\"\n","working_dir = base_dir + \"FeatureLearning/\"\n","\n","src_dir = working_dir + \"src/\"\n","corpus_dir = src_dir + \"bsc/\"\n","cons_dir = corpus_dir + \"consonants/\"\n","full_seg_dir = corpus_dir + \"fullchunk/\"\n","full_sample_dir = corpus_dir + \"fullchunksample/\"\n","\n","save_dir = working_dir + \"modelsave/english/\"\n","\n","train_name = full_sample_dir + \"fullchunk\"\n","\n","tags_name = full_seg_dir + \"tags\"\n","test_name = full_seg_dir + \"fullchunk\""],"metadata":{"id":"iGouCDYD3h18"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#################################################################\n","#             Constants              #\n","#################################################################\n","EPOCHS = 10\n","BATCH_SIZE = 128\n","\n","FRAMES_IN_SEGMENT = 25\n","SEGMENTS_IN_CHUNK = 100  # set_size\n","\n","MFCC_DIM = 13\n","INPUT_DIM = FRAMES_IN_SEGMENT * MFCC_DIM * 3\n","OUTPUT_DIM = FRAMES_IN_SEGMENT * MFCC_DIM\n","# let's still maintain these interdims but just ignore them when building the model \n","INTER_DIM_1 = 256\n","INTER_DIM_2 = 64\n","INTER_DIM_3 = 16\n","LATENT_DIM = 3\n","\n","DROPOUT = 0.5"],"metadata":{"id":"u4etuFQOAsMu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#################################################################\n","#              Utils               #\n","#################################################################\n","def get_timestamp():\n","    # for model save\n","    return dt.datetime.now().strftime(\"%m%d%H%M%S\")   # timestamp ignores year and second, not really needed. "],"metadata":{"id":"AaVefMr_AXH3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To construct own PyTorch Dataset instance, need to specify:\n","# - how to retrieve a data entry\n","# - how to process a data entry \n","class UngroundedSoundDataset(Dataset): \n","    def __init__(self, train_name, chunks, suffix) :\n","        # Now we enable the dataset loader to read in multiple chunks at a time and and can catenate them until one single piece\n","        self.dataset = None\n","        for chunk in chunks: \n","            path = train_name + chunk + suffix\n","            if self.dataset is None: \n","                self.dataset = np.load(path)\n","            else: \n","                self.dataset = np.concatenate((self.dataset, np.load(path)))\n","        \n","    # REQUIRED: provide size of dataset (= #images)\n","    def __len__(self) :\n","        return self.dataset.shape[0]\n","        # return sum(self.clip_meta[\"chunk_size\"].tolist())\n","\n","    def __getitem__(self, idx): \n","        inp = torch.from_numpy(self.dataset[idx]).to(torch.float32)\n","        outp = inp[:, :13]\n","        return inp, outp"],"metadata":{"id":"0LsZNqTOQU4w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To construct own PyTorch Dataset instance, need to specify:\n","# - how to retrieve a data entry\n","# - how to process a data entry \n","class GroundedSoundDataset(Dataset): \n","    def __init__(self, tags, chunk, full_chunk=True, set_size=50) :\n","        \"\"\"\n","        Custom PyTorch Dataset class to load and process image data and labels.\n","        For each input data entry:\n","        1. Calculate the chunk num and within-chunk idx from the given idx\n","        2. Load that chunk from disk and get out the corresponding segment\n","        3. Although we're using autoencoder, according to Shain & Elsner (2019), \n","            the input and output dimensions are not exactly the same. \n","            The input includes mfcc, delta and its second order delta, \n","            whereas the output only has mfccs. Since the deltas could be \n","            calculated from mfccs, there is no need to learn to output them. \n","            → the get function need to prepare two copies of segments: 39dim and 13dim. \n","\n","        Note that a chunk includes segments, a segment includes n (25) frames of mfccs. \n","\n","        Inputs: \n","            clip_meta: metadata in pandas dataframe format. Including file_general_name and chunk_size. \n","            chunksdir: directory of mfcc chunk files\n","            set_size: number of segments in a standard-size chunk. default to 50. \n","        \"\"\"\n","        # Store inputs for later use\n","        # we suppose that clip_meta is ordered in chunk_size, \n","        # therefore standard chunks are always together and after nonstandard chunks. \n","        self.tags = tags\n","        if full_chunk: \n","            self.dataset = np.load(chunk)\n","            self.dataset_tags = self.tags[\"tag\"]\n","            assert self.dataset.shape[0] == self.dataset_tags.shape[0]\n","        else: \n","            pass    # try first, not filled in. \n","        \n","    # REQUIRED: provide size of dataset (= #images)\n","    def __len__(self) :\n","        return self.dataset.shape[0]\n","        # return sum(self.clip_meta[\"chunk_size\"].tolist())\n","\n","    def __getitem__(self, idx): \n","        inp = torch.from_numpy(self.dataset[idx]).to(torch.float32)\n","        outp = inp[:, :13]\n","        return inp, outp, self.dataset_tags[idx]"],"metadata":{"id":"hAr9WK8F7fyS"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E7R4BFye1eAW"},"source":["class ResBlock(nn.Module):\n","    def __init__(self, n_chans):\n","        super(ResBlock, self).__init__()\n","        self.lin1 = nn.Linear(n_chans, n_chans)\n","        self.lin2 = nn.Linear(n_chans, n_chans)\n","        self.batch_norm = nn.BatchNorm1d(num_features=n_chans)  # <5>\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        out = self.lin1(x)\n","        out = self.batch_norm(out)\n","        out = self.relu(out)\n","        out = self.lin2(out)\n","        out = self.batch_norm(out)\n","        out = self.relu(out)\n","        return out + x\n","\n","class LinPack(nn.Module):\n","    def __init__(self, n_in, n_out):\n","        super(LinPack, self).__init__()\n","        self.lin = nn.Linear(n_in, n_out)\n","        self.relu = nn.ReLU()\n","        self.batch_norm = nn.BatchNorm1d(num_features=n_out)\n","        # self.dropout = nn.Dropout(p=DROPOUT)\n","\n","    def forward(self, x):\n","        x = self.lin(x)\n","        x = self.relu(x)\n","        x = self.batch_norm(x)\n","        # x = self.dropout(x)\n","        return x\n","\n","\n","class ResAE(nn.Module):\n","    def __init__(self, input_dim=INPUT_DIM, inter_dim1=INTER_DIM_1, inter_dim2=INTER_DIM_2, inter_dim3=INTER_DIM_3, latent_dim=LATENT_DIM, output_dim=OUTPUT_DIM):\n","        super(ResAE, self).__init__()\n","\n","        self.encoder = nn.Sequential(\n","            LinPack(input_dim, inter_dim1), \n","            ResBlock(inter_dim1), \n","            # ResBlock(inter_dim1), \n","            nn.Linear(inter_dim1, latent_dim), \n","            # nn.Sigmoid()\n","        )\n","\n","        self.decoder =  nn.Sequential(\n","            LinPack(latent_dim, inter_dim1), \n","            ResBlock(inter_dim1), \n","            # ResBlock(inter_dim1), \n","            nn.Linear(inter_dim1, output_dim),\n","            # nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        org_size = x.size()\n","        y_size = (org_size[0], org_size[1], org_size[2] // 3)\n","        batch = org_size[0]\n","        x = x.view(batch, -1)\n","\n","        h = self.encoder(x)\n","        recon_x = self.decoder(h).view(size=y_size)\n","\n","        return recon_x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-4xlUuq1hDx"},"source":["recon_loss = nn.MSELoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lUxoYBUg1jLq"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = ResAE(INPUT_DIM, INTER_DIM_1, INTER_DIM_2, INTER_DIM_3, LATENT_DIM, OUTPUT_DIM)\n","model.to(device)\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QZBCTRw3iXys","executionInfo":{"status":"ok","timestamp":1675766054010,"user_tz":-480,"elapsed":10,"user":{"displayName":"Frank Learning","userId":"10777113402383858671"}},"outputId":"7947acdb-1a95-49a4-8b1d-93f442cf41d1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResAE(\n","  (encoder): Sequential(\n","    (0): LinPack(\n","      (lin): Linear(in_features=975, out_features=256, bias=True)\n","      (relu): ReLU()\n","      (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): ResBlock(\n","      (lin1): Linear(in_features=256, out_features=256, bias=True)\n","      (lin2): Linear(in_features=256, out_features=256, bias=True)\n","      (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","    (2): Linear(in_features=256, out_features=3, bias=True)\n","  )\n","  (decoder): Sequential(\n","    (0): LinPack(\n","      (lin): Linear(in_features=3, out_features=256, bias=True)\n","      (relu): ReLU()\n","      (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): ResBlock(\n","      (lin1): Linear(in_features=256, out_features=256, bias=True)\n","      (lin2): Linear(in_features=256, out_features=256, bias=True)\n","      (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","    (2): Linear(in_features=256, out_features=325, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Define recorders of training hists, for ease of extension\n","class Recorder: \n","    def __init__(self, IOPath): \n","        self.record = []\n","        self.IOPath = IOPath\n","\n","    def save(self): \n","        pass\n","    \n","    def append(self, content): \n","        self.record.append(content)\n","    \n","    def get(self): \n","        return self.record"],"metadata":{"id":"NNHDmuigs8OB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LossRecorder(Recorder): \n","    def read(self): \n","        # only used by loss hists \n","        with open(self.IOPath, 'rb') as f:\n","            self.record = pickle.load(f)\n","    \n","    def save(self): \n","        with open(self.IOPath, 'wb') as file:\n","            pickle.dump(self.record, file)\n","\n","\n","class HistRecorder(Recorder):     \n","    def save(self): \n","        with open(self.IOPath, \"a\") as txt:\n","            txt.write(\"\\n\".join(self.record))\n","    \n","    def print(self, content): \n","        self.append(content)\n","        print(content)"],"metadata":{"id":"kGMfle47t3Hj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Just for keeping records of training hists. \n","# ts = str(get_timestamp())\n","ts = \"0130021416\"\n","save_txt_name = \"train_txt_{}.hst\".format(ts)\n","save_trainhist_name = \"train_hist_{}.hst\".format(ts)\n","save_valhist_name = \"val_hist_{}.hst\".format(ts)"],"metadata":{"id":"ofsEE6OaoyPh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["valid_losses = LossRecorder(save_dir + save_valhist_name)\n","train_losses = LossRecorder(save_dir + save_trainhist_name)\n","text_hist = HistRecorder(save_dir + save_txt_name)"],"metadata":{"id":"xUHYarigvT64"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["READ = False\n","# READ = True"],"metadata":{"id":"-T4OYaoXsxe_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if READ: \n","    valid_losses.read()\n","    train_losses.read()\n","\n","    # model_name = last_model_name\n","    model_name = \"model_english_0130021416_9_full\"\n","    model_path = save_dir + model_name + \".pt\"\n","    state = torch.load(model_path)\n","    model = ResAE()\n","    model.load_state_dict(state)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)"],"metadata":{"id":"nVvnpUk5sWxb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_loss = 1e9\n","best_epoch = 0\n","EPOCHS = 30\n","BASE = 0\n","train_seqs = [\"_06_10\", \"_16_20\", \"_21_25\", \"_26_30\", \"_31_35\"]   # \"_31_35\"\n","test_seqs = [\"_01_05\"]\n","last_model_name = \"\""],"metadata":{"id":"PpIGGnVWvLW-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_gsds = UngroundedSoundDataset(train_name, train_seqs, \".npy\")\n","# train_loader = DataLoader(train_gsds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n","# train_num = len(train_loader.dataset)\n","\n","valid_gsds = UngroundedSoundDataset(train_name, test_seqs, \".npy\")\n","valid_loader = DataLoader(valid_gsds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n","valid_num = len(valid_loader.dataset)"],"metadata":{"id":"6OCx4nqP40fz"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y2n7doAD1uRi","executionInfo":{"status":"ok","timestamp":1675061210677,"user_tz":-480,"elapsed":3002536,"user":{"displayName":"Frank Learning","userId":"10777113402383858671"}},"outputId":"e9c5bcb7-72db-4238-e83f-36e4dbe35748"},"source":["for epoch in range(BASE, BASE + EPOCHS):\n","    text_hist.print(\"Epoch {}\".format(epoch))\n","\n","    model.train()\n","    train_loss = 0.\n","    train_num = 0. \n","    base = 0\n","    for chunk in train_seqs: \n","        train_gsds = UngroundedSoundDataset(train_name, [chunk], \".npy\")\n","        train_loader = DataLoader(train_gsds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n","        train_num += len(train_loader.dataset)\n","        for idx, (x, y) in enumerate(train_loader):\n","            idx += base    # for idx counting, not for other use\n","            batch = x.size(0)\n","            x = x.to(device)\n","            y = y.to(device)\n","            recon_x = model(x)\n","            recon = recon_loss(recon_x, y)\n","\n","            loss = recon\n","            train_loss += loss.item()\n","            loss = loss / batch\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            if idx % 100 == 0:\n","                text_hist.print(f\"Training loss {loss: .3f} in Step {idx}\")\n","        base += len(train_loader)\n","\n","    train_losses.append(train_loss / train_num)\n","    text_hist.print(f\"※※※Training loss {train_loss / train_num: .3f}※※※\")\n","\n","    torch.save(model.state_dict(), 'model_english')\n","    last_model_name = \"model_english_{}_{}_full\".format(ts, epoch)\n","    shutil.move(root_dir + \"model_english\", save_dir + last_model_name + \".pt\")\n","    text_hist.print(\"Training timepoint saved\")\n","\n","    model.eval()\n","    valid_loss = 0.\n","\n","    for idx, (x, y) in enumerate(valid_loader):\n","        batch = x.size(0)\n","        x = x.to(device)\n","        y = y.to(device)\n","        recon_x = model(x)\n","        recon = recon_loss(recon_x, y)\n","\n","        loss = recon\n","        valid_loss += loss.item()\n","        loss = loss / batch\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if idx % 100 == 0:\n","            # \\t Recon {recon / batch: .3f} \\t KL {kl / batch: .3f}\n","            text_hist.print(f\"Valid loss {loss: .3f} in Step {idx}\")\n","\n","    valid_losses.append(valid_loss / valid_num)\n","    text_hist.print(f\"Valid loss {valid_loss / valid_num: .3f}※※※\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0\n","Training loss  1.060 in Step 0\n","Training loss  0.508 in Step 100\n","Training loss  1.167 in Step 200\n","Training loss  1.015 in Step 300\n","Training loss  0.856 in Step 400\n","Training loss  0.894 in Step 500\n","Training loss  0.939 in Step 600\n","Training loss  0.675 in Step 700\n","Training loss  0.618 in Step 800\n","Training loss  0.796 in Step 900\n","Training loss  0.493 in Step 1000\n","Training loss  0.557 in Step 1100\n","Training loss  0.711 in Step 1200\n","Training loss  0.621 in Step 1300\n","Training loss  0.682 in Step 1400\n","Training loss  0.800 in Step 1500\n","Training loss  0.676 in Step 1600\n","Training loss  0.644 in Step 1700\n","Training loss  0.719 in Step 1800\n","Training loss  0.910 in Step 1900\n","Training loss  0.773 in Step 2000\n","Training loss  0.538 in Step 2100\n","Training loss  0.680 in Step 2200\n","Training loss  0.613 in Step 2300\n","Training loss  0.753 in Step 2400\n","Training loss  0.638 in Step 2500\n","Training loss  0.669 in Step 2600\n","Training loss  0.739 in Step 2700\n","Training loss  0.668 in Step 2800\n","Training loss  1.055 in Step 2900\n","Training loss  0.440 in Step 3000\n","Training loss  0.425 in Step 3100\n","Training loss  0.653 in Step 3200\n","Training loss  0.734 in Step 3300\n","Training loss  0.567 in Step 3400\n","Training loss  1.013 in Step 3500\n","Training loss  0.729 in Step 3600\n","Training loss  0.672 in Step 3700\n","Training loss  0.960 in Step 3800\n","Training loss  0.610 in Step 3900\n","Training loss  0.568 in Step 4000\n","Training loss  0.621 in Step 4100\n","Training loss  0.701 in Step 4200\n","Training loss  0.519 in Step 4300\n","Training loss  0.515 in Step 4400\n","Training loss  0.620 in Step 4500\n","Training loss  0.654 in Step 4600\n","Training loss  0.583 in Step 4700\n","Training loss  0.674 in Step 4800\n","Training loss  0.783 in Step 4900\n","Training loss  0.450 in Step 5000\n","Training loss  0.458 in Step 5100\n","Training loss  0.491 in Step 5200\n","Training loss  0.523 in Step 5300\n","Training loss  0.568 in Step 5400\n","Training loss  0.556 in Step 5500\n","Training loss  0.595 in Step 5600\n","Training loss  0.690 in Step 5700\n","Training loss  0.617 in Step 5800\n","Training loss  0.713 in Step 5900\n","Training loss  0.688 in Step 6000\n","Training loss  0.604 in Step 6100\n","Training loss  0.940 in Step 6200\n","Training loss  0.637 in Step 6300\n","Training loss  0.821 in Step 6400\n","Training loss  0.790 in Step 6500\n","Training loss  0.666 in Step 6600\n","Training loss  0.542 in Step 6700\n","Training loss  0.519 in Step 6800\n","Training loss  0.463 in Step 6900\n","Training loss  0.604 in Step 7000\n","Training loss  0.516 in Step 7100\n","Training loss  0.594 in Step 7200\n","※※※Training loss  0.666※※※\n","Training timepoint saved\n","Valid loss  0.801 in Step 0\n","Valid loss  0.447 in Step 100\n","Valid loss  0.632 in Step 200\n","Valid loss  0.900 in Step 300\n","Valid loss  0.490 in Step 400\n","Valid loss  0.528 in Step 500\n","Valid loss  0.906 in Step 600\n","Valid loss  0.611 in Step 700\n","Valid loss  0.555 in Step 800\n","Valid loss  0.641 in Step 900\n","Valid loss  0.879 in Step 1000\n","Valid loss  0.514 in Step 1100\n","Valid loss  0.568 in Step 1200\n","Valid loss  0.411 in Step 1300\n","Valid loss  0.261 in Step 1400\n","Valid loss  0.285 in Step 1500\n","Valid loss  0.444 in Step 1600\n","Valid loss  0.614※※※\n","Epoch 1\n","Training loss  0.688 in Step 0\n","Training loss  0.416 in Step 100\n","Training loss  1.030 in Step 200\n","Training loss  0.890 in Step 300\n","Training loss  0.751 in Step 400\n","Training loss  0.835 in Step 500\n","Training loss  0.822 in Step 600\n","Training loss  0.587 in Step 700\n","Training loss  0.549 in Step 800\n","Training loss  0.694 in Step 900\n","Training loss  0.432 in Step 1000\n","Training loss  0.532 in Step 1100\n","Training loss  0.639 in Step 1200\n","Training loss  0.586 in Step 1300\n","Training loss  0.655 in Step 1400\n","Training loss  0.737 in Step 1500\n","Training loss  0.665 in Step 1600\n","Training loss  0.618 in Step 1700\n","Training loss  0.689 in Step 1800\n","Training loss  0.845 in Step 1900\n","Training loss  0.742 in Step 2000\n","Training loss  0.524 in Step 2100\n","Training loss  0.655 in Step 2200\n","Training loss  0.584 in Step 2300\n","Training loss  0.721 in Step 2400\n","Training loss  0.622 in Step 2500\n","Training loss  0.644 in Step 2600\n","Training loss  0.721 in Step 2700\n","Training loss  0.664 in Step 2800\n","Training loss  1.001 in Step 2900\n","Training loss  0.433 in Step 3000\n","Training loss  0.389 in Step 3100\n","Training loss  0.635 in Step 3200\n","Training loss  0.683 in Step 3300\n","Training loss  0.560 in Step 3400\n","Training loss  0.868 in Step 3500\n","Training loss  0.703 in Step 3600\n","Training loss  0.628 in Step 3700\n","Training loss  0.953 in Step 3800\n","Training loss  0.587 in Step 3900\n","Training loss  0.546 in Step 4000\n","Training loss  0.590 in Step 4100\n","Training loss  0.664 in Step 4200\n","Training loss  0.486 in Step 4300\n","Training loss  0.457 in Step 4400\n","Training loss  0.614 in Step 4500\n","Training loss  0.637 in Step 4600\n","Training loss  0.550 in Step 4700\n","Training loss  0.654 in Step 4800\n","Training loss  0.728 in Step 4900\n","Training loss  0.461 in Step 5000\n","Training loss  0.432 in Step 5100\n","Training loss  0.464 in Step 5200\n","Training loss  0.487 in Step 5300\n","Training loss  0.498 in Step 5400\n","Training loss  0.549 in Step 5500\n","Training loss  0.583 in Step 5600\n","Training loss  0.673 in Step 5700\n","Training loss  0.579 in Step 5800\n","Training loss  0.685 in Step 5900\n","Training loss  0.671 in Step 6000\n","Training loss  0.582 in Step 6100\n","Training loss  0.925 in Step 6200\n","Training loss  0.645 in Step 6300\n","Training loss  0.775 in Step 6400\n","Training loss  0.749 in Step 6500\n","Training loss  0.639 in Step 6600\n","Training loss  0.520 in Step 6700\n","Training loss  0.502 in Step 6800\n","Training loss  0.447 in Step 6900\n","Training loss  0.579 in Step 7000\n","Training loss  0.502 in Step 7100\n","Training loss  0.579 in Step 7200\n","※※※Training loss  0.627※※※\n","Training timepoint saved\n","Valid loss  0.697 in Step 0\n","Valid loss  0.423 in Step 100\n","Valid loss  0.591 in Step 200\n","Valid loss  0.860 in Step 300\n","Valid loss  0.488 in Step 400\n","Valid loss  0.510 in Step 500\n","Valid loss  0.875 in Step 600\n","Valid loss  0.595 in Step 700\n","Valid loss  0.545 in Step 800\n","Valid loss  0.633 in Step 900\n","Valid loss  0.824 in Step 1000\n","Valid loss  0.476 in Step 1100\n","Valid loss  0.554 in Step 1200\n","Valid loss  0.402 in Step 1300\n","Valid loss  0.257 in Step 1400\n","Valid loss  0.277 in Step 1500\n","Valid loss  0.435 in Step 1600\n","Valid loss  0.594※※※\n","Epoch 2\n","Training loss  0.656 in Step 0\n","Training loss  0.400 in Step 100\n","Training loss  0.988 in Step 200\n","Training loss  0.860 in Step 300\n","Training loss  0.731 in Step 400\n","Training loss  0.818 in Step 500\n","Training loss  0.774 in Step 600\n","Training loss  0.573 in Step 700\n","Training loss  0.521 in Step 800\n","Training loss  0.649 in Step 900\n","Training loss  0.371 in Step 1000\n","Training loss  0.517 in Step 1100\n","Training loss  0.624 in Step 1200\n","Training loss  0.571 in Step 1300\n","Training loss  0.639 in Step 1400\n","Training loss  0.726 in Step 1500\n","Training loss  0.636 in Step 1600\n","Training loss  0.594 in Step 1700\n","Training loss  0.674 in Step 1800\n","Training loss  0.812 in Step 1900\n","Training loss  0.734 in Step 2000\n","Training loss  0.533 in Step 2100\n","Training loss  0.648 in Step 2200\n","Training loss  0.571 in Step 2300\n","Training loss  0.697 in Step 2400\n","Training loss  0.605 in Step 2500\n","Training loss  0.630 in Step 2600\n","Training loss  0.680 in Step 2700\n","Training loss  0.649 in Step 2800\n","Training loss  0.981 in Step 2900\n","Training loss  0.432 in Step 3000\n","Training loss  0.352 in Step 3100\n","Training loss  0.612 in Step 3200\n","Training loss  0.683 in Step 3300\n","Training loss  0.566 in Step 3400\n","Training loss  0.772 in Step 3500\n","Training loss  0.694 in Step 3600\n","Training loss  0.621 in Step 3700\n","Training loss  0.928 in Step 3800\n","Training loss  0.580 in Step 3900\n","Training loss  0.535 in Step 4000\n","Training loss  0.590 in Step 4100\n","Training loss  0.650 in Step 4200\n","Training loss  0.480 in Step 4300\n","Training loss  0.439 in Step 4400\n","Training loss  0.595 in Step 4500\n","Training loss  0.621 in Step 4600\n","Training loss  0.547 in Step 4700\n","Training loss  0.633 in Step 4800\n","Training loss  0.717 in Step 4900\n","Training loss  0.425 in Step 5000\n","Training loss  0.412 in Step 5100\n","Training loss  0.452 in Step 5200\n","Training loss  0.482 in Step 5300\n","Training loss  0.468 in Step 5400\n","Training loss  0.534 in Step 5500\n","Training loss  0.577 in Step 5600\n","Training loss  0.674 in Step 5700\n","Training loss  0.563 in Step 5800\n","Training loss  0.659 in Step 5900\n","Training loss  0.660 in Step 6000\n","Training loss  0.561 in Step 6100\n","Training loss  0.894 in Step 6200\n","Training loss  0.630 in Step 6300\n","Training loss  0.743 in Step 6400\n","Training loss  0.740 in Step 6500\n","Training loss  0.621 in Step 6600\n","Training loss  0.519 in Step 6700\n","Training loss  0.484 in Step 6800\n","Training loss  0.445 in Step 6900\n","Training loss  0.571 in Step 7000\n","Training loss  0.492 in Step 7100\n","Training loss  0.575 in Step 7200\n","※※※Training loss  0.610※※※\n","Training timepoint saved\n","Valid loss  0.777 in Step 0\n","Valid loss  0.424 in Step 100\n","Valid loss  0.571 in Step 200\n","Valid loss  0.825 in Step 300\n","Valid loss  0.485 in Step 400\n","Valid loss  0.497 in Step 500\n","Valid loss  0.874 in Step 600\n","Valid loss  0.588 in Step 700\n","Valid loss  0.542 in Step 800\n","Valid loss  0.620 in Step 900\n","Valid loss  0.804 in Step 1000\n","Valid loss  0.465 in Step 1100\n","Valid loss  0.540 in Step 1200\n","Valid loss  0.389 in Step 1300\n","Valid loss  0.252 in Step 1400\n","Valid loss  0.280 in Step 1500\n","Valid loss  0.429 in Step 1600\n","Valid loss  0.585※※※\n","Epoch 3\n","Training loss  0.549 in Step 0\n","Training loss  0.384 in Step 100\n","Training loss  0.944 in Step 200\n","Training loss  0.834 in Step 300\n","Training loss  0.712 in Step 400\n","Training loss  0.813 in Step 500\n","Training loss  0.759 in Step 600\n","Training loss  0.555 in Step 700\n","Training loss  0.517 in Step 800\n","Training loss  0.631 in Step 900\n","Training loss  0.332 in Step 1000\n","Training loss  0.501 in Step 1100\n","Training loss  0.610 in Step 1200\n","Training loss  0.562 in Step 1300\n","Training loss  0.610 in Step 1400\n","Training loss  0.722 in Step 1500\n","Training loss  0.623 in Step 1600\n","Training loss  0.591 in Step 1700\n","Training loss  0.665 in Step 1800\n","Training loss  0.802 in Step 1900\n","Training loss  0.710 in Step 2000\n","Training loss  0.527 in Step 2100\n","Training loss  0.651 in Step 2200\n","Training loss  0.568 in Step 2300\n","Training loss  0.692 in Step 2400\n","Training loss  0.605 in Step 2500\n","Training loss  0.566 in Step 2600\n","Training loss  0.625 in Step 2700\n","Training loss  0.646 in Step 2800\n","Training loss  0.948 in Step 2900\n","Training loss  0.425 in Step 3000\n","Training loss  0.326 in Step 3100\n","Training loss  0.593 in Step 3200\n","Training loss  0.687 in Step 3300\n","Training loss  0.562 in Step 3400\n","Training loss  0.747 in Step 3500\n","Training loss  0.683 in Step 3600\n","Training loss  0.623 in Step 3700\n","Training loss  0.890 in Step 3800\n","Training loss  0.563 in Step 3900\n","Training loss  0.529 in Step 4000\n","Training loss  0.564 in Step 4100\n","Training loss  0.643 in Step 4200\n","Training loss  0.478 in Step 4300\n","Training loss  0.423 in Step 4400\n","Training loss  0.584 in Step 4500\n","Training loss  0.621 in Step 4600\n","Training loss  0.514 in Step 4700\n","Training loss  0.625 in Step 4800\n","Training loss  0.707 in Step 4900\n","Training loss  0.427 in Step 5000\n","Training loss  0.402 in Step 5100\n","Training loss  0.454 in Step 5200\n","Training loss  0.472 in Step 5300\n","Training loss  0.469 in Step 5400\n","Training loss  0.536 in Step 5500\n","Training loss  0.557 in Step 5600\n","Training loss  0.660 in Step 5700\n","Training loss  0.549 in Step 5800\n","Training loss  0.638 in Step 5900\n","Training loss  0.645 in Step 6000\n","Training loss  0.568 in Step 6100\n","Training loss  0.868 in Step 6200\n","Training loss  0.605 in Step 6300\n","Training loss  0.732 in Step 6400\n","Training loss  0.735 in Step 6500\n","Training loss  0.612 in Step 6600\n","Training loss  0.520 in Step 6700\n","Training loss  0.479 in Step 6800\n","Training loss  0.444 in Step 6900\n","Training loss  0.572 in Step 7000\n","Training loss  0.484 in Step 7100\n","Training loss  0.577 in Step 7200\n","※※※Training loss  0.598※※※\n","Training timepoint saved\n","Valid loss  0.697 in Step 0\n","Valid loss  0.444 in Step 100\n","Valid loss  0.567 in Step 200\n","Valid loss  0.833 in Step 300\n","Valid loss  0.507 in Step 400\n","Valid loss  0.504 in Step 500\n","Valid loss  0.870 in Step 600\n","Valid loss  0.581 in Step 700\n","Valid loss  0.538 in Step 800\n","Valid loss  0.604 in Step 900\n","Valid loss  0.801 in Step 1000\n","Valid loss  0.468 in Step 1100\n","Valid loss  0.536 in Step 1200\n","Valid loss  0.400 in Step 1300\n","Valid loss  0.255 in Step 1400\n","Valid loss  0.270 in Step 1500\n","Valid loss  0.437 in Step 1600\n","Valid loss  0.585※※※\n","Epoch 4\n","Training loss  0.589 in Step 0\n","Training loss  0.373 in Step 100\n","Training loss  0.920 in Step 200\n","Training loss  0.810 in Step 300\n","Training loss  0.706 in Step 400\n","Training loss  0.790 in Step 500\n","Training loss  0.753 in Step 600\n","Training loss  0.549 in Step 700\n","Training loss  0.511 in Step 800\n","Training loss  0.614 in Step 900\n","Training loss  0.326 in Step 1000\n","Training loss  0.497 in Step 1100\n","Training loss  0.601 in Step 1200\n","Training loss  0.557 in Step 1300\n","Training loss  0.602 in Step 1400\n","Training loss  0.720 in Step 1500\n","Training loss  0.604 in Step 1600\n","Training loss  0.588 in Step 1700\n","Training loss  0.665 in Step 1800\n","Training loss  0.772 in Step 1900\n","Training loss  0.700 in Step 2000\n","Training loss  0.515 in Step 2100\n","Training loss  0.644 in Step 2200\n","Training loss  0.569 in Step 2300\n","Training loss  0.676 in Step 2400\n","Training loss  0.616 in Step 2500\n","Training loss  0.527 in Step 2600\n","Training loss  0.590 in Step 2700\n","Training loss  0.622 in Step 2800\n","Training loss  0.928 in Step 2900\n","Training loss  0.428 in Step 3000\n","Training loss  0.326 in Step 3100\n","Training loss  0.581 in Step 3200\n","Training loss  0.687 in Step 3300\n","Training loss  0.545 in Step 3400\n","Training loss  0.725 in Step 3500\n","Training loss  0.666 in Step 3600\n","Training loss  0.614 in Step 3700\n","Training loss  0.850 in Step 3800\n","Training loss  0.539 in Step 3900\n","Training loss  0.531 in Step 4000\n","Training loss  0.559 in Step 4100\n","Training loss  0.628 in Step 4200\n","Training loss  0.474 in Step 4300\n","Training loss  0.421 in Step 4400\n","Training loss  0.582 in Step 4500\n","Training loss  0.615 in Step 4600\n","Training loss  0.522 in Step 4700\n","Training loss  0.613 in Step 4800\n","Training loss  0.704 in Step 4900\n","Training loss  0.419 in Step 5000\n","Training loss  0.399 in Step 5100\n","Training loss  0.446 in Step 5200\n","Training loss  0.473 in Step 5300\n","Training loss  0.468 in Step 5400\n","Training loss  0.528 in Step 5500\n","Training loss  0.553 in Step 5600\n","Training loss  0.649 in Step 5700\n","Training loss  0.546 in Step 5800\n","Training loss  0.621 in Step 5900\n","Training loss  0.619 in Step 6000\n","Training loss  0.557 in Step 6100\n","Training loss  0.846 in Step 6200\n","Training loss  0.583 in Step 6300\n","Training loss  0.720 in Step 6400\n","Training loss  0.727 in Step 6500\n","Training loss  0.612 in Step 6600\n","Training loss  0.510 in Step 6700\n","Training loss  0.470 in Step 6800\n","Training loss  0.441 in Step 6900\n","Training loss  0.564 in Step 7000\n","Training loss  0.483 in Step 7100\n","Training loss  0.575 in Step 7200\n","※※※Training loss  0.589※※※\n","Training timepoint saved\n","Valid loss  0.679 in Step 0\n","Valid loss  0.411 in Step 100\n","Valid loss  0.561 in Step 200\n","Valid loss  0.806 in Step 300\n","Valid loss  0.488 in Step 400\n","Valid loss  0.492 in Step 500\n","Valid loss  0.853 in Step 600\n","Valid loss  0.582 in Step 700\n","Valid loss  0.530 in Step 800\n","Valid loss  0.600 in Step 900\n","Valid loss  0.785 in Step 1000\n","Valid loss  0.458 in Step 1100\n","Valid loss  0.529 in Step 1200\n","Valid loss  0.407 in Step 1300\n","Valid loss  0.248 in Step 1400\n","Valid loss  0.268 in Step 1500\n","Valid loss  0.440 in Step 1600\n","Valid loss  0.572※※※\n","Epoch 5\n","Training loss  0.514 in Step 0\n","Training loss  0.373 in Step 100\n","Training loss  0.905 in Step 200\n","Training loss  0.790 in Step 300\n","Training loss  0.695 in Step 400\n","Training loss  0.790 in Step 500\n","Training loss  0.733 in Step 600\n","Training loss  0.544 in Step 700\n","Training loss  0.503 in Step 800\n","Training loss  0.602 in Step 900\n","Training loss  0.327 in Step 1000\n","Training loss  0.481 in Step 1100\n","Training loss  0.591 in Step 1200\n","Training loss  0.551 in Step 1300\n","Training loss  0.602 in Step 1400\n","Training loss  0.729 in Step 1500\n","Training loss  0.586 in Step 1600\n","Training loss  0.586 in Step 1700\n","Training loss  0.658 in Step 1800\n","Training loss  0.767 in Step 1900\n","Training loss  0.702 in Step 2000\n","Training loss  0.500 in Step 2100\n","Training loss  0.634 in Step 2200\n","Training loss  0.569 in Step 2300\n","Training loss  0.664 in Step 2400\n","Training loss  0.602 in Step 2500\n","Training loss  0.525 in Step 2600\n","Training loss  0.588 in Step 2700\n","Training loss  0.597 in Step 2800\n","Training loss  0.903 in Step 2900\n","Training loss  0.420 in Step 3000\n","Training loss  0.302 in Step 3100\n","Training loss  0.579 in Step 3200\n","Training loss  0.668 in Step 3300\n","Training loss  0.553 in Step 3400\n","Training loss  0.722 in Step 3500\n","Training loss  0.655 in Step 3600\n","Training loss  0.605 in Step 3700\n","Training loss  0.807 in Step 3800\n","Training loss  0.539 in Step 3900\n","Training loss  0.519 in Step 4000\n","Training loss  0.510 in Step 4100\n","Training loss  0.627 in Step 4200\n","Training loss  0.470 in Step 4300\n","Training loss  0.413 in Step 4400\n","Training loss  0.579 in Step 4500\n","Training loss  0.604 in Step 4600\n","Training loss  0.517 in Step 4700\n","Training loss  0.616 in Step 4800\n","Training loss  0.699 in Step 4900\n","Training loss  0.417 in Step 5000\n","Training loss  0.391 in Step 5100\n","Training loss  0.447 in Step 5200\n","Training loss  0.456 in Step 5300\n","Training loss  0.466 in Step 5400\n","Training loss  0.521 in Step 5500\n","Training loss  0.560 in Step 5600\n","Training loss  0.650 in Step 5700\n","Training loss  0.537 in Step 5800\n","Training loss  0.617 in Step 5900\n","Training loss  0.632 in Step 6000\n","Training loss  0.550 in Step 6100\n","Training loss  0.852 in Step 6200\n","Training loss  0.582 in Step 6300\n","Training loss  0.711 in Step 6400\n","Training loss  0.713 in Step 6500\n","Training loss  0.605 in Step 6600\n","Training loss  0.507 in Step 6700\n","Training loss  0.462 in Step 6800\n","Training loss  0.431 in Step 6900\n","Training loss  0.555 in Step 7000\n","Training loss  0.475 in Step 7100\n","Training loss  0.576 in Step 7200\n","※※※Training loss  0.580※※※\n","Training timepoint saved\n","Valid loss  0.675 in Step 0\n","Valid loss  0.404 in Step 100\n","Valid loss  0.555 in Step 200\n","Valid loss  0.813 in Step 300\n","Valid loss  0.478 in Step 400\n","Valid loss  0.490 in Step 500\n","Valid loss  0.845 in Step 600\n","Valid loss  0.589 in Step 700\n","Valid loss  0.535 in Step 800\n","Valid loss  0.600 in Step 900\n","Valid loss  0.785 in Step 1000\n","Valid loss  0.446 in Step 1100\n","Valid loss  0.530 in Step 1200\n","Valid loss  0.395 in Step 1300\n","Valid loss  0.240 in Step 1400\n","Valid loss  0.262 in Step 1500\n","Valid loss  0.432 in Step 1600\n","Valid loss  0.568※※※\n","Epoch 6\n","Training loss  0.500 in Step 0\n","Training loss  0.354 in Step 100\n","Training loss  0.900 in Step 200\n","Training loss  0.777 in Step 300\n","Training loss  0.694 in Step 400\n","Training loss  0.763 in Step 500\n","Training loss  0.721 in Step 600\n","Training loss  0.545 in Step 700\n","Training loss  0.492 in Step 800\n","Training loss  0.597 in Step 900\n","Training loss  0.309 in Step 1000\n","Training loss  0.461 in Step 1100\n","Training loss  0.585 in Step 1200\n","Training loss  0.537 in Step 1300\n","Training loss  0.598 in Step 1400\n","Training loss  0.709 in Step 1500\n","Training loss  0.574 in Step 1600\n","Training loss  0.571 in Step 1700\n","Training loss  0.662 in Step 1800\n","Training loss  0.756 in Step 1900\n","Training loss  0.695 in Step 2000\n","Training loss  0.495 in Step 2100\n","Training loss  0.644 in Step 2200\n","Training loss  0.564 in Step 2300\n","Training loss  0.663 in Step 2400\n","Training loss  0.599 in Step 2500\n","Training loss  0.505 in Step 2600\n","Training loss  0.584 in Step 2700\n","Training loss  0.596 in Step 2800\n","Training loss  0.889 in Step 2900\n","Training loss  0.424 in Step 3000\n","Training loss  0.295 in Step 3100\n","Training loss  0.575 in Step 3200\n","Training loss  0.679 in Step 3300\n","Training loss  0.530 in Step 3400\n","Training loss  0.666 in Step 3500\n","Training loss  0.658 in Step 3600\n","Training loss  0.621 in Step 3700\n","Training loss  0.797 in Step 3800\n","Training loss  0.522 in Step 3900\n","Training loss  0.526 in Step 4000\n","Training loss  0.521 in Step 4100\n","Training loss  0.627 in Step 4200\n","Training loss  0.465 in Step 4300\n","Training loss  0.414 in Step 4400\n","Training loss  0.582 in Step 4500\n","Training loss  0.601 in Step 4600\n","Training loss  0.490 in Step 4700\n","Training loss  0.613 in Step 4800\n","Training loss  0.692 in Step 4900\n","Training loss  0.418 in Step 5000\n","Training loss  0.388 in Step 5100\n","Training loss  0.445 in Step 5200\n","Training loss  0.459 in Step 5300\n","Training loss  0.458 in Step 5400\n","Training loss  0.513 in Step 5500\n","Training loss  0.557 in Step 5600\n","Training loss  0.628 in Step 5700\n","Training loss  0.537 in Step 5800\n","Training loss  0.612 in Step 5900\n","Training loss  0.631 in Step 6000\n","Training loss  0.542 in Step 6100\n","Training loss  0.835 in Step 6200\n","Training loss  0.571 in Step 6300\n","Training loss  0.692 in Step 6400\n","Training loss  0.707 in Step 6500\n","Training loss  0.598 in Step 6600\n","Training loss  0.495 in Step 6700\n","Training loss  0.445 in Step 6800\n","Training loss  0.434 in Step 6900\n","Training loss  0.553 in Step 7000\n","Training loss  0.478 in Step 7100\n","Training loss  0.564 in Step 7200\n","※※※Training loss  0.573※※※\n","Training timepoint saved\n","Valid loss  0.591 in Step 0\n","Valid loss  0.405 in Step 100\n","Valid loss  0.551 in Step 200\n","Valid loss  0.787 in Step 300\n","Valid loss  0.476 in Step 400\n","Valid loss  0.473 in Step 500\n","Valid loss  0.834 in Step 600\n","Valid loss  0.561 in Step 700\n","Valid loss  0.541 in Step 800\n","Valid loss  0.599 in Step 900\n","Valid loss  0.780 in Step 1000\n","Valid loss  0.438 in Step 1100\n","Valid loss  0.516 in Step 1200\n","Valid loss  0.387 in Step 1300\n","Valid loss  0.246 in Step 1400\n","Valid loss  0.280 in Step 1500\n","Valid loss  0.425 in Step 1600\n","Valid loss  0.563※※※\n","Epoch 7\n","Training loss  0.508 in Step 0\n","Training loss  0.351 in Step 100\n","Training loss  0.896 in Step 200\n","Training loss  0.776 in Step 300\n","Training loss  0.700 in Step 400\n","Training loss  0.771 in Step 500\n","Training loss  0.708 in Step 600\n","Training loss  0.541 in Step 700\n","Training loss  0.493 in Step 800\n","Training loss  0.587 in Step 900\n","Training loss  0.304 in Step 1000\n","Training loss  0.459 in Step 1100\n","Training loss  0.588 in Step 1200\n","Training loss  0.546 in Step 1300\n","Training loss  0.591 in Step 1400\n","Training loss  0.700 in Step 1500\n","Training loss  0.580 in Step 1600\n","Training loss  0.577 in Step 1700\n","Training loss  0.651 in Step 1800\n","Training loss  0.742 in Step 1900\n","Training loss  0.679 in Step 2000\n","Training loss  0.484 in Step 2100\n","Training loss  0.625 in Step 2200\n","Training loss  0.561 in Step 2300\n","Training loss  0.658 in Step 2400\n","Training loss  0.600 in Step 2500\n","Training loss  0.503 in Step 2600\n","Training loss  0.573 in Step 2700\n","Training loss  0.585 in Step 2800\n","Training loss  0.882 in Step 2900\n","Training loss  0.418 in Step 3000\n","Training loss  0.295 in Step 3100\n","Training loss  0.558 in Step 3200\n","Training loss  0.655 in Step 3300\n","Training loss  0.527 in Step 3400\n","Training loss  0.646 in Step 3500\n","Training loss  0.660 in Step 3600\n","Training loss  0.610 in Step 3700\n","Training loss  0.750 in Step 3800\n","Training loss  0.517 in Step 3900\n","Training loss  0.516 in Step 4000\n","Training loss  0.514 in Step 4100\n","Training loss  0.620 in Step 4200\n","Training loss  0.457 in Step 4300\n","Training loss  0.409 in Step 4400\n","Training loss  0.569 in Step 4500\n","Training loss  0.594 in Step 4600\n","Training loss  0.491 in Step 4700\n","Training loss  0.608 in Step 4800\n","Training loss  0.691 in Step 4900\n","Training loss  0.416 in Step 5000\n","Training loss  0.376 in Step 5100\n","Training loss  0.445 in Step 5200\n","Training loss  0.450 in Step 5300\n","Training loss  0.457 in Step 5400\n","Training loss  0.504 in Step 5500\n","Training loss  0.551 in Step 5600\n","Training loss  0.633 in Step 5700\n","Training loss  0.519 in Step 5800\n","Training loss  0.604 in Step 5900\n","Training loss  0.617 in Step 6000\n","Training loss  0.536 in Step 6100\n","Training loss  0.837 in Step 6200\n","Training loss  0.569 in Step 6300\n","Training loss  0.695 in Step 6400\n","Training loss  0.693 in Step 6500\n","Training loss  0.596 in Step 6600\n","Training loss  0.490 in Step 6700\n","Training loss  0.443 in Step 6800\n","Training loss  0.436 in Step 6900\n","Training loss  0.556 in Step 7000\n","Training loss  0.469 in Step 7100\n","Training loss  0.562 in Step 7200\n","※※※Training loss  0.568※※※\n","Training timepoint saved\n","Valid loss  0.671 in Step 0\n","Valid loss  0.409 in Step 100\n","Valid loss  0.548 in Step 200\n","Valid loss  0.788 in Step 300\n","Valid loss  0.474 in Step 400\n","Valid loss  0.468 in Step 500\n","Valid loss  0.826 in Step 600\n","Valid loss  0.563 in Step 700\n","Valid loss  0.533 in Step 800\n","Valid loss  0.593 in Step 900\n","Valid loss  0.782 in Step 1000\n","Valid loss  0.444 in Step 1100\n","Valid loss  0.508 in Step 1200\n","Valid loss  0.370 in Step 1300\n","Valid loss  0.242 in Step 1400\n","Valid loss  0.270 in Step 1500\n","Valid loss  0.431 in Step 1600\n","Valid loss  0.559※※※\n","Epoch 8\n","Training loss  0.496 in Step 0\n","Training loss  0.354 in Step 100\n","Training loss  0.858 in Step 200\n","Training loss  0.765 in Step 300\n","Training loss  0.689 in Step 400\n","Training loss  0.753 in Step 500\n","Training loss  0.710 in Step 600\n","Training loss  0.546 in Step 700\n","Training loss  0.487 in Step 800\n","Training loss  0.575 in Step 900\n","Training loss  0.302 in Step 1000\n","Training loss  0.458 in Step 1100\n","Training loss  0.579 in Step 1200\n","Training loss  0.528 in Step 1300\n","Training loss  0.586 in Step 1400\n","Training loss  0.706 in Step 1500\n","Training loss  0.567 in Step 1600\n","Training loss  0.564 in Step 1700\n","Training loss  0.639 in Step 1800\n","Training loss  0.730 in Step 1900\n","Training loss  0.687 in Step 2000\n","Training loss  0.480 in Step 2100\n","Training loss  0.626 in Step 2200\n","Training loss  0.566 in Step 2300\n","Training loss  0.639 in Step 2400\n","Training loss  0.589 in Step 2500\n","Training loss  0.516 in Step 2600\n","Training loss  0.556 in Step 2700\n","Training loss  0.568 in Step 2800\n","Training loss  0.868 in Step 2900\n","Training loss  0.414 in Step 3000\n","Training loss  0.296 in Step 3100\n","Training loss  0.575 in Step 3200\n","Training loss  0.670 in Step 3300\n","Training loss  0.535 in Step 3400\n","Training loss  0.602 in Step 3500\n","Training loss  0.648 in Step 3600\n","Training loss  0.603 in Step 3700\n","Training loss  0.743 in Step 3800\n","Training loss  0.513 in Step 3900\n","Training loss  0.516 in Step 4000\n","Training loss  0.507 in Step 4100\n","Training loss  0.624 in Step 4200\n","Training loss  0.459 in Step 4300\n","Training loss  0.415 in Step 4400\n","Training loss  0.568 in Step 4500\n","Training loss  0.599 in Step 4600\n","Training loss  0.477 in Step 4700\n","Training loss  0.594 in Step 4800\n","Training loss  0.688 in Step 4900\n","Training loss  0.409 in Step 5000\n","Training loss  0.373 in Step 5100\n","Training loss  0.442 in Step 5200\n","Training loss  0.457 in Step 5300\n","Training loss  0.455 in Step 5400\n","Training loss  0.507 in Step 5500\n","Training loss  0.556 in Step 5600\n","Training loss  0.627 in Step 5700\n","Training loss  0.526 in Step 5800\n","Training loss  0.613 in Step 5900\n","Training loss  0.610 in Step 6000\n","Training loss  0.538 in Step 6100\n","Training loss  0.815 in Step 6200\n","Training loss  0.571 in Step 6300\n","Training loss  0.691 in Step 6400\n","Training loss  0.690 in Step 6500\n","Training loss  0.593 in Step 6600\n","Training loss  0.481 in Step 6700\n","Training loss  0.447 in Step 6800\n","Training loss  0.433 in Step 6900\n","Training loss  0.542 in Step 7000\n","Training loss  0.470 in Step 7100\n","Training loss  0.565 in Step 7200\n","※※※Training loss  0.563※※※\n","Training timepoint saved\n","Valid loss  0.639 in Step 0\n","Valid loss  0.408 in Step 100\n","Valid loss  0.550 in Step 200\n","Valid loss  0.786 in Step 300\n","Valid loss  0.477 in Step 400\n","Valid loss  0.470 in Step 500\n","Valid loss  0.829 in Step 600\n","Valid loss  0.558 in Step 700\n","Valid loss  0.530 in Step 800\n","Valid loss  0.598 in Step 900\n","Valid loss  0.757 in Step 1000\n","Valid loss  0.435 in Step 1100\n","Valid loss  0.506 in Step 1200\n","Valid loss  0.380 in Step 1300\n","Valid loss  0.242 in Step 1400\n","Valid loss  0.273 in Step 1500\n","Valid loss  0.421 in Step 1600\n","Valid loss  0.558※※※\n","Epoch 9\n","Training loss  0.449 in Step 0\n","Training loss  0.353 in Step 100\n","Training loss  0.845 in Step 200\n","Training loss  0.750 in Step 300\n","Training loss  0.686 in Step 400\n","Training loss  0.748 in Step 500\n","Training loss  0.706 in Step 600\n","Training loss  0.534 in Step 700\n","Training loss  0.486 in Step 800\n","Training loss  0.583 in Step 900\n","Training loss  0.300 in Step 1000\n","Training loss  0.451 in Step 1100\n","Training loss  0.581 in Step 1200\n","Training loss  0.532 in Step 1300\n","Training loss  0.586 in Step 1400\n","Training loss  0.684 in Step 1500\n","Training loss  0.573 in Step 1600\n","Training loss  0.566 in Step 1700\n","Training loss  0.640 in Step 1800\n","Training loss  0.731 in Step 1900\n","Training loss  0.679 in Step 2000\n","Training loss  0.481 in Step 2100\n","Training loss  0.633 in Step 2200\n","Training loss  0.556 in Step 2300\n","Training loss  0.631 in Step 2400\n","Training loss  0.602 in Step 2500\n","Training loss  0.516 in Step 2600\n","Training loss  0.547 in Step 2700\n","Training loss  0.580 in Step 2800\n","Training loss  0.868 in Step 2900\n","Training loss  0.417 in Step 3000\n","Training loss  0.283 in Step 3100\n","Training loss  0.562 in Step 3200\n","Training loss  0.660 in Step 3300\n","Training loss  0.532 in Step 3400\n","Training loss  0.611 in Step 3500\n","Training loss  0.651 in Step 3600\n","Training loss  0.607 in Step 3700\n","Training loss  0.748 in Step 3800\n","Training loss  0.516 in Step 3900\n","Training loss  0.515 in Step 4000\n","Training loss  0.484 in Step 4100\n","Training loss  0.625 in Step 4200\n","Training loss  0.459 in Step 4300\n","Training loss  0.404 in Step 4400\n","Training loss  0.568 in Step 4500\n","Training loss  0.588 in Step 4600\n","Training loss  0.488 in Step 4700\n","Training loss  0.591 in Step 4800\n","Training loss  0.693 in Step 4900\n","Training loss  0.405 in Step 5000\n","Training loss  0.372 in Step 5100\n","Training loss  0.444 in Step 5200\n","Training loss  0.443 in Step 5300\n","Training loss  0.454 in Step 5400\n","Training loss  0.505 in Step 5500\n","Training loss  0.554 in Step 5600\n","Training loss  0.623 in Step 5700\n","Training loss  0.517 in Step 5800\n","Training loss  0.610 in Step 5900\n","Training loss  0.608 in Step 6000\n","Training loss  0.544 in Step 6100\n","Training loss  0.834 in Step 6200\n","Training loss  0.555 in Step 6300\n","Training loss  0.684 in Step 6400\n","Training loss  0.692 in Step 6500\n","Training loss  0.590 in Step 6600\n","Training loss  0.481 in Step 6700\n","Training loss  0.445 in Step 6800\n","Training loss  0.431 in Step 6900\n","Training loss  0.548 in Step 7000\n","Training loss  0.466 in Step 7100\n","Training loss  0.571 in Step 7200\n","※※※Training loss  0.559※※※\n","Training timepoint saved\n","Valid loss  0.619 in Step 0\n","Valid loss  0.403 in Step 100\n","Valid loss  0.551 in Step 200\n","Valid loss  0.767 in Step 300\n","Valid loss  0.468 in Step 400\n","Valid loss  0.457 in Step 500\n","Valid loss  0.829 in Step 600\n","Valid loss  0.552 in Step 700\n","Valid loss  0.539 in Step 800\n","Valid loss  0.589 in Step 900\n","Valid loss  0.775 in Step 1000\n","Valid loss  0.436 in Step 1100\n","Valid loss  0.500 in Step 1200\n","Valid loss  0.370 in Step 1300\n","Valid loss  0.245 in Step 1400\n","Valid loss  0.260 in Step 1500\n","Valid loss  0.430 in Step 1600\n","Valid loss  0.555※※※\n","Epoch 10\n","Training loss  0.438 in Step 0\n","Training loss  0.364 in Step 100\n","Training loss  0.827 in Step 200\n","Training loss  0.746 in Step 300\n","Training loss  0.677 in Step 400\n","Training loss  0.733 in Step 500\n","Training loss  0.693 in Step 600\n","Training loss  0.537 in Step 700\n","Training loss  0.488 in Step 800\n","Training loss  0.577 in Step 900\n","Training loss  0.296 in Step 1000\n","Training loss  0.445 in Step 1100\n","Training loss  0.574 in Step 1200\n","Training loss  0.518 in Step 1300\n","Training loss  0.582 in Step 1400\n","Training loss  0.675 in Step 1500\n","Training loss  0.568 in Step 1600\n","Training loss  0.551 in Step 1700\n","Training loss  0.638 in Step 1800\n","Training loss  0.722 in Step 1900\n","Training loss  0.670 in Step 2000\n","Training loss  0.470 in Step 2100\n","Training loss  0.623 in Step 2200\n","Training loss  0.558 in Step 2300\n","Training loss  0.622 in Step 2400\n","Training loss  0.587 in Step 2500\n","Training loss  0.494 in Step 2600\n","Training loss  0.548 in Step 2700\n","Training loss  0.560 in Step 2800\n","Training loss  0.852 in Step 2900\n","Training loss  0.415 in Step 3000\n","Training loss  0.281 in Step 3100\n","Training loss  0.557 in Step 3200\n","Training loss  0.663 in Step 3300\n","Training loss  0.525 in Step 3400\n","Training loss  0.623 in Step 3500\n","Training loss  0.644 in Step 3600\n","Training loss  0.607 in Step 3700\n","Training loss  0.732 in Step 3800\n","Training loss  0.522 in Step 3900\n","Training loss  0.509 in Step 4000\n","Training loss  0.488 in Step 4100\n","Training loss  0.622 in Step 4200\n","Training loss  0.447 in Step 4300\n","Training loss  0.406 in Step 4400\n","Training loss  0.558 in Step 4500\n","Training loss  0.589 in Step 4600\n","Training loss  0.477 in Step 4700\n","Training loss  0.591 in Step 4800\n","Training loss  0.683 in Step 4900\n","Training loss  0.409 in Step 5000\n","Training loss  0.373 in Step 5100\n","Training loss  0.442 in Step 5200\n","Training loss  0.451 in Step 5300\n","Training loss  0.439 in Step 5400\n","Training loss  0.502 in Step 5500\n","Training loss  0.555 in Step 5600\n","Training loss  0.620 in Step 5700\n","Training loss  0.520 in Step 5800\n","Training loss  0.612 in Step 5900\n","Training loss  0.611 in Step 6000\n","Training loss  0.536 in Step 6100\n","Training loss  0.828 in Step 6200\n","Training loss  0.563 in Step 6300\n","Training loss  0.672 in Step 6400\n","Training loss  0.687 in Step 6500\n","Training loss  0.587 in Step 6600\n","Training loss  0.477 in Step 6700\n","Training loss  0.445 in Step 6800\n","Training loss  0.424 in Step 6900\n","Training loss  0.546 in Step 7000\n","Training loss  0.462 in Step 7100\n","Training loss  0.559 in Step 7200\n","※※※Training loss  0.556※※※\n","Training timepoint saved\n","Valid loss  0.625 in Step 0\n","Valid loss  0.405 in Step 100\n","Valid loss  0.548 in Step 200\n","Valid loss  0.776 in Step 300\n","Valid loss  0.470 in Step 400\n","Valid loss  0.462 in Step 500\n","Valid loss  0.820 in Step 600\n","Valid loss  0.553 in Step 700\n","Valid loss  0.525 in Step 800\n","Valid loss  0.592 in Step 900\n","Valid loss  0.764 in Step 1000\n","Valid loss  0.428 in Step 1100\n","Valid loss  0.495 in Step 1200\n","Valid loss  0.357 in Step 1300\n","Valid loss  0.239 in Step 1400\n","Valid loss  0.270 in Step 1500\n","Valid loss  0.425 in Step 1600\n","Valid loss  0.552※※※\n","Epoch 11\n","Training loss  0.462 in Step 0\n","Training loss  0.343 in Step 100\n","Training loss  0.826 in Step 200\n","Training loss  0.741 in Step 300\n","Training loss  0.674 in Step 400\n","Training loss  0.729 in Step 500\n","Training loss  0.688 in Step 600\n","Training loss  0.531 in Step 700\n","Training loss  0.495 in Step 800\n","Training loss  0.576 in Step 900\n","Training loss  0.297 in Step 1000\n","Training loss  0.449 in Step 1100\n","Training loss  0.578 in Step 1200\n","Training loss  0.524 in Step 1300\n","Training loss  0.573 in Step 1400\n","Training loss  0.670 in Step 1500\n","Training loss  0.565 in Step 1600\n","Training loss  0.561 in Step 1700\n","Training loss  0.627 in Step 1800\n","Training loss  0.724 in Step 1900\n","Training loss  0.671 in Step 2000\n","Training loss  0.478 in Step 2100\n","Training loss  0.626 in Step 2200\n","Training loss  0.546 in Step 2300\n","Training loss  0.618 in Step 2400\n","Training loss  0.597 in Step 2500\n","Training loss  0.510 in Step 2600\n","Training loss  0.541 in Step 2700\n","Training loss  0.561 in Step 2800\n","Training loss  0.825 in Step 2900\n","Training loss  0.419 in Step 3000\n","Training loss  0.286 in Step 3100\n","Training loss  0.559 in Step 3200\n","Training loss  0.684 in Step 3300\n","Training loss  0.532 in Step 3400\n","Training loss  0.569 in Step 3500\n","Training loss  0.642 in Step 3600\n","Training loss  0.603 in Step 3700\n","Training loss  0.724 in Step 3800\n","Training loss  0.524 in Step 3900\n","Training loss  0.510 in Step 4000\n","Training loss  0.483 in Step 4100\n","Training loss  0.610 in Step 4200\n","Training loss  0.454 in Step 4300\n","Training loss  0.405 in Step 4400\n","Training loss  0.573 in Step 4500\n","Training loss  0.585 in Step 4600\n","Training loss  0.475 in Step 4700\n","Training loss  0.597 in Step 4800\n","Training loss  0.699 in Step 4900\n","Training loss  0.397 in Step 5000\n","Training loss  0.372 in Step 5100\n","Training loss  0.440 in Step 5200\n","Training loss  0.439 in Step 5300\n","Training loss  0.444 in Step 5400\n","Training loss  0.517 in Step 5500\n","Training loss  0.552 in Step 5600\n","Training loss  0.640 in Step 5700\n","Training loss  0.510 in Step 5800\n","Training loss  0.603 in Step 5900\n","Training loss  0.592 in Step 6000\n","Training loss  0.526 in Step 6100\n","Training loss  0.819 in Step 6200\n","Training loss  0.559 in Step 6300\n","Training loss  0.674 in Step 6400\n","Training loss  0.677 in Step 6500\n","Training loss  0.587 in Step 6600\n","Training loss  0.474 in Step 6700\n","Training loss  0.441 in Step 6800\n","Training loss  0.430 in Step 6900\n","Training loss  0.547 in Step 7000\n","Training loss  0.468 in Step 7100\n","Training loss  0.564 in Step 7200\n","※※※Training loss  0.553※※※\n","Training timepoint saved\n","Valid loss  0.638 in Step 0\n","Valid loss  0.397 in Step 100\n","Valid loss  0.546 in Step 200\n","Valid loss  0.767 in Step 300\n","Valid loss  0.466 in Step 400\n","Valid loss  0.467 in Step 500\n","Valid loss  0.832 in Step 600\n","Valid loss  0.565 in Step 700\n","Valid loss  0.525 in Step 800\n","Valid loss  0.584 in Step 900\n","Valid loss  0.753 in Step 1000\n","Valid loss  0.435 in Step 1100\n","Valid loss  0.496 in Step 1200\n","Valid loss  0.372 in Step 1300\n","Valid loss  0.239 in Step 1400\n","Valid loss  0.263 in Step 1500\n","Valid loss  0.420 in Step 1600\n","Valid loss  0.551※※※\n","Epoch 12\n","Training loss  0.461 in Step 0\n","Training loss  0.359 in Step 100\n","Training loss  0.816 in Step 200\n","Training loss  0.725 in Step 300\n","Training loss  0.671 in Step 400\n","Training loss  0.743 in Step 500\n","Training loss  0.686 in Step 600\n","Training loss  0.529 in Step 700\n","Training loss  0.501 in Step 800\n","Training loss  0.567 in Step 900\n","Training loss  0.300 in Step 1000\n","Training loss  0.443 in Step 1100\n","Training loss  0.571 in Step 1200\n","Training loss  0.527 in Step 1300\n","Training loss  0.566 in Step 1400\n","Training loss  0.666 in Step 1500\n","Training loss  0.567 in Step 1600\n","Training loss  0.572 in Step 1700\n","Training loss  0.627 in Step 1800\n","Training loss  0.724 in Step 1900\n","Training loss  0.667 in Step 2000\n","Training loss  0.465 in Step 2100\n","Training loss  0.620 in Step 2200\n","Training loss  0.555 in Step 2300\n","Training loss  0.614 in Step 2400\n","Training loss  0.592 in Step 2500\n","Training loss  0.497 in Step 2600\n","Training loss  0.534 in Step 2700\n","Training loss  0.550 in Step 2800\n","Training loss  0.838 in Step 2900\n","Training loss  0.415 in Step 3000\n","Training loss  0.282 in Step 3100\n","Training loss  0.551 in Step 3200\n","Training loss  0.652 in Step 3300\n","Training loss  0.536 in Step 3400\n","Training loss  0.577 in Step 3500\n","Training loss  0.643 in Step 3600\n","Training loss  0.602 in Step 3700\n","Training loss  0.707 in Step 3800\n","Training loss  0.519 in Step 3900\n","Training loss  0.501 in Step 4000\n","Training loss  0.497 in Step 4100\n","Training loss  0.609 in Step 4200\n","Training loss  0.454 in Step 4300\n","Training loss  0.410 in Step 4400\n","Training loss  0.562 in Step 4500\n","Training loss  0.585 in Step 4600\n","Training loss  0.473 in Step 4700\n","Training loss  0.591 in Step 4800\n","Training loss  0.686 in Step 4900\n","Training loss  0.394 in Step 5000\n","Training loss  0.371 in Step 5100\n","Training loss  0.435 in Step 5200\n","Training loss  0.437 in Step 5300\n","Training loss  0.443 in Step 5400\n","Training loss  0.487 in Step 5500\n","Training loss  0.556 in Step 5600\n","Training loss  0.624 in Step 5700\n","Training loss  0.505 in Step 5800\n","Training loss  0.603 in Step 5900\n","Training loss  0.595 in Step 6000\n","Training loss  0.541 in Step 6100\n","Training loss  0.807 in Step 6200\n","Training loss  0.560 in Step 6300\n","Training loss  0.661 in Step 6400\n","Training loss  0.670 in Step 6500\n","Training loss  0.579 in Step 6600\n","Training loss  0.481 in Step 6700\n","Training loss  0.438 in Step 6800\n","Training loss  0.429 in Step 6900\n","Training loss  0.554 in Step 7000\n","Training loss  0.462 in Step 7100\n","Training loss  0.560 in Step 7200\n","※※※Training loss  0.551※※※\n","Training timepoint saved\n","Valid loss  0.668 in Step 0\n","Valid loss  0.396 in Step 100\n","Valid loss  0.551 in Step 200\n","Valid loss  0.768 in Step 300\n","Valid loss  0.464 in Step 400\n","Valid loss  0.466 in Step 500\n","Valid loss  0.827 in Step 600\n","Valid loss  0.561 in Step 700\n","Valid loss  0.521 in Step 800\n","Valid loss  0.581 in Step 900\n","Valid loss  0.757 in Step 1000\n","Valid loss  0.452 in Step 1100\n","Valid loss  0.500 in Step 1200\n","Valid loss  0.372 in Step 1300\n","Valid loss  0.243 in Step 1400\n","Valid loss  0.256 in Step 1500\n","Valid loss  0.420 in Step 1600\n","Valid loss  0.551※※※\n","Epoch 13\n","Training loss  0.461 in Step 0\n","Training loss  0.341 in Step 100\n","Training loss  0.801 in Step 200\n","Training loss  0.732 in Step 300\n","Training loss  0.672 in Step 400\n","Training loss  0.736 in Step 500\n","Training loss  0.690 in Step 600\n","Training loss  0.529 in Step 700\n","Training loss  0.479 in Step 800\n","Training loss  0.566 in Step 900\n","Training loss  0.296 in Step 1000\n","Training loss  0.436 in Step 1100\n","Training loss  0.577 in Step 1200\n","Training loss  0.506 in Step 1300\n","Training loss  0.575 in Step 1400\n","Training loss  0.671 in Step 1500\n","Training loss  0.567 in Step 1600\n","Training loss  0.561 in Step 1700\n","Training loss  0.619 in Step 1800\n","Training loss  0.723 in Step 1900\n","Training loss  0.660 in Step 2000\n","Training loss  0.469 in Step 2100\n","Training loss  0.626 in Step 2200\n","Training loss  0.548 in Step 2300\n","Training loss  0.619 in Step 2400\n","Training loss  0.593 in Step 2500\n","Training loss  0.507 in Step 2600\n","Training loss  0.531 in Step 2700\n","Training loss  0.546 in Step 2800\n","Training loss  0.817 in Step 2900\n","Training loss  0.413 in Step 3000\n","Training loss  0.282 in Step 3100\n","Training loss  0.563 in Step 3200\n","Training loss  0.645 in Step 3300\n","Training loss  0.531 in Step 3400\n","Training loss  0.615 in Step 3500\n","Training loss  0.639 in Step 3600\n","Training loss  0.603 in Step 3700\n","Training loss  0.699 in Step 3800\n","Training loss  0.512 in Step 3900\n","Training loss  0.517 in Step 4000\n","Training loss  0.471 in Step 4100\n","Training loss  0.610 in Step 4200\n","Training loss  0.454 in Step 4300\n","Training loss  0.405 in Step 4400\n","Training loss  0.560 in Step 4500\n","Training loss  0.583 in Step 4600\n","Training loss  0.477 in Step 4700\n","Training loss  0.594 in Step 4800\n","Training loss  0.679 in Step 4900\n","Training loss  0.401 in Step 5000\n","Training loss  0.373 in Step 5100\n","Training loss  0.436 in Step 5200\n","Training loss  0.437 in Step 5300\n","Training loss  0.436 in Step 5400\n","Training loss  0.492 in Step 5500\n","Training loss  0.547 in Step 5600\n","Training loss  0.620 in Step 5700\n","Training loss  0.508 in Step 5800\n","Training loss  0.598 in Step 5900\n","Training loss  0.580 in Step 6000\n","Training loss  0.532 in Step 6100\n","Training loss  0.792 in Step 6200\n","Training loss  0.552 in Step 6300\n","Training loss  0.655 in Step 6400\n","Training loss  0.670 in Step 6500\n","Training loss  0.579 in Step 6600\n","Training loss  0.481 in Step 6700\n","Training loss  0.428 in Step 6800\n","Training loss  0.417 in Step 6900\n","Training loss  0.555 in Step 7000\n","Training loss  0.455 in Step 7100\n","Training loss  0.559 in Step 7200\n","※※※Training loss  0.548※※※\n","Training timepoint saved\n","Valid loss  0.658 in Step 0\n","Valid loss  0.391 in Step 100\n","Valid loss  0.545 in Step 200\n","Valid loss  0.755 in Step 300\n","Valid loss  0.456 in Step 400\n","Valid loss  0.462 in Step 500\n","Valid loss  0.801 in Step 600\n","Valid loss  0.548 in Step 700\n","Valid loss  0.525 in Step 800\n","Valid loss  0.602 in Step 900\n","Valid loss  0.731 in Step 1000\n","Valid loss  0.436 in Step 1100\n","Valid loss  0.489 in Step 1200\n","Valid loss  0.377 in Step 1300\n","Valid loss  0.239 in Step 1400\n","Valid loss  0.260 in Step 1500\n","Valid loss  0.417 in Step 1600\n","Valid loss  0.547※※※\n","Epoch 14\n","Training loss  0.397 in Step 0\n","Training loss  0.333 in Step 100\n","Training loss  0.776 in Step 200\n","Training loss  0.731 in Step 300\n","Training loss  0.669 in Step 400\n","Training loss  0.744 in Step 500\n","Training loss  0.693 in Step 600\n","Training loss  0.527 in Step 700\n","Training loss  0.482 in Step 800\n","Training loss  0.561 in Step 900\n","Training loss  0.291 in Step 1000\n","Training loss  0.441 in Step 1100\n","Training loss  0.567 in Step 1200\n","Training loss  0.511 in Step 1300\n","Training loss  0.577 in Step 1400\n","Training loss  0.655 in Step 1500\n","Training loss  0.571 in Step 1600\n","Training loss  0.577 in Step 1700\n","Training loss  0.629 in Step 1800\n","Training loss  0.717 in Step 1900\n","Training loss  0.660 in Step 2000\n","Training loss  0.466 in Step 2100\n","Training loss  0.627 in Step 2200\n","Training loss  0.547 in Step 2300\n","Training loss  0.601 in Step 2400\n","Training loss  0.580 in Step 2500\n","Training loss  0.502 in Step 2600\n","Training loss  0.526 in Step 2700\n","Training loss  0.556 in Step 2800\n","Training loss  0.826 in Step 2900\n","Training loss  0.408 in Step 3000\n","Training loss  0.282 in Step 3100\n","Training loss  0.558 in Step 3200\n","Training loss  0.652 in Step 3300\n","Training loss  0.531 in Step 3400\n","Training loss  0.609 in Step 3500\n","Training loss  0.638 in Step 3600\n","Training loss  0.609 in Step 3700\n","Training loss  0.692 in Step 3800\n","Training loss  0.521 in Step 3900\n","Training loss  0.501 in Step 4000\n","Training loss  0.474 in Step 4100\n","Training loss  0.607 in Step 4200\n","Training loss  0.456 in Step 4300\n","Training loss  0.402 in Step 4400\n","Training loss  0.556 in Step 4500\n","Training loss  0.583 in Step 4600\n","Training loss  0.474 in Step 4700\n","Training loss  0.594 in Step 4800\n","Training loss  0.674 in Step 4900\n","Training loss  0.398 in Step 5000\n","Training loss  0.361 in Step 5100\n","Training loss  0.431 in Step 5200\n","Training loss  0.438 in Step 5300\n","Training loss  0.430 in Step 5400\n","Training loss  0.491 in Step 5500\n","Training loss  0.550 in Step 5600\n","Training loss  0.618 in Step 5700\n","Training loss  0.507 in Step 5800\n","Training loss  0.597 in Step 5900\n","Training loss  0.574 in Step 6000\n","Training loss  0.512 in Step 6100\n","Training loss  0.794 in Step 6200\n","Training loss  0.541 in Step 6300\n","Training loss  0.653 in Step 6400\n","Training loss  0.655 in Step 6500\n","Training loss  0.583 in Step 6600\n","Training loss  0.473 in Step 6700\n","Training loss  0.431 in Step 6800\n","Training loss  0.425 in Step 6900\n","Training loss  0.556 in Step 7000\n","Training loss  0.452 in Step 7100\n","Training loss  0.558 in Step 7200\n","※※※Training loss  0.546※※※\n","Training timepoint saved\n","Valid loss  0.650 in Step 0\n","Valid loss  0.393 in Step 100\n","Valid loss  0.554 in Step 200\n","Valid loss  0.759 in Step 300\n","Valid loss  0.455 in Step 400\n","Valid loss  0.456 in Step 500\n","Valid loss  0.821 in Step 600\n","Valid loss  0.556 in Step 700\n","Valid loss  0.517 in Step 800\n","Valid loss  0.577 in Step 900\n","Valid loss  0.753 in Step 1000\n","Valid loss  0.430 in Step 1100\n","Valid loss  0.482 in Step 1200\n","Valid loss  0.360 in Step 1300\n","Valid loss  0.237 in Step 1400\n","Valid loss  0.256 in Step 1500\n","Valid loss  0.419 in Step 1600\n","Valid loss  0.545※※※\n","Epoch 15\n","Training loss  0.438 in Step 0\n","Training loss  0.338 in Step 100\n","Training loss  0.769 in Step 200\n","Training loss  0.716 in Step 300\n","Training loss  0.666 in Step 400\n","Training loss  0.723 in Step 500\n","Training loss  0.690 in Step 600\n","Training loss  0.523 in Step 700\n","Training loss  0.483 in Step 800\n","Training loss  0.558 in Step 900\n","Training loss  0.294 in Step 1000\n","Training loss  0.447 in Step 1100\n","Training loss  0.558 in Step 1200\n","Training loss  0.505 in Step 1300\n","Training loss  0.578 in Step 1400\n","Training loss  0.665 in Step 1500\n","Training loss  0.577 in Step 1600\n","Training loss  0.567 in Step 1700\n","Training loss  0.627 in Step 1800\n","Training loss  0.709 in Step 1900\n","Training loss  0.662 in Step 2000\n","Training loss  0.463 in Step 2100\n","Training loss  0.618 in Step 2200\n","Training loss  0.549 in Step 2300\n","Training loss  0.609 in Step 2400\n","Training loss  0.573 in Step 2500\n","Training loss  0.492 in Step 2600\n","Training loss  0.530 in Step 2700\n","Training loss  0.555 in Step 2800\n","Training loss  0.815 in Step 2900\n","Training loss  0.407 in Step 3000\n","Training loss  0.275 in Step 3100\n","Training loss  0.552 in Step 3200\n","Training loss  0.631 in Step 3300\n","Training loss  0.526 in Step 3400\n","Training loss  0.585 in Step 3500\n","Training loss  0.630 in Step 3600\n","Training loss  0.594 in Step 3700\n","Training loss  0.695 in Step 3800\n","Training loss  0.497 in Step 3900\n","Training loss  0.507 in Step 4000\n","Training loss  0.472 in Step 4100\n","Training loss  0.606 in Step 4200\n","Training loss  0.454 in Step 4300\n","Training loss  0.406 in Step 4400\n","Training loss  0.566 in Step 4500\n","Training loss  0.585 in Step 4600\n","Training loss  0.474 in Step 4700\n","Training loss  0.590 in Step 4800\n","Training loss  0.670 in Step 4900\n","Training loss  0.388 in Step 5000\n","Training loss  0.364 in Step 5100\n","Training loss  0.437 in Step 5200\n","Training loss  0.444 in Step 5300\n","Training loss  0.435 in Step 5400\n","Training loss  0.485 in Step 5500\n","Training loss  0.545 in Step 5600\n","Training loss  0.619 in Step 5700\n","Training loss  0.501 in Step 5800\n","Training loss  0.600 in Step 5900\n","Training loss  0.582 in Step 6000\n","Training loss  0.518 in Step 6100\n","Training loss  0.792 in Step 6200\n","Training loss  0.548 in Step 6300\n","Training loss  0.666 in Step 6400\n","Training loss  0.656 in Step 6500\n","Training loss  0.569 in Step 6600\n","Training loss  0.467 in Step 6700\n","Training loss  0.430 in Step 6800\n","Training loss  0.422 in Step 6900\n","Training loss  0.553 in Step 7000\n","Training loss  0.456 in Step 7100\n","Training loss  0.555 in Step 7200\n","※※※Training loss  0.545※※※\n","Training timepoint saved\n","Valid loss  0.684 in Step 0\n","Valid loss  0.391 in Step 100\n","Valid loss  0.543 in Step 200\n","Valid loss  0.758 in Step 300\n","Valid loss  0.450 in Step 400\n","Valid loss  0.461 in Step 500\n","Valid loss  0.805 in Step 600\n","Valid loss  0.538 in Step 700\n","Valid loss  0.516 in Step 800\n","Valid loss  0.588 in Step 900\n","Valid loss  0.737 in Step 1000\n","Valid loss  0.421 in Step 1100\n","Valid loss  0.478 in Step 1200\n","Valid loss  0.374 in Step 1300\n","Valid loss  0.234 in Step 1400\n","Valid loss  0.259 in Step 1500\n","Valid loss  0.415 in Step 1600\n","Valid loss  0.545※※※\n","Epoch 16\n","Training loss  0.408 in Step 0\n","Training loss  0.331 in Step 100\n","Training loss  0.783 in Step 200\n","Training loss  0.724 in Step 300\n","Training loss  0.662 in Step 400\n","Training loss  0.715 in Step 500\n","Training loss  0.685 in Step 600\n","Training loss  0.518 in Step 700\n","Training loss  0.476 in Step 800\n","Training loss  0.566 in Step 900\n","Training loss  0.297 in Step 1000\n","Training loss  0.437 in Step 1100\n","Training loss  0.560 in Step 1200\n","Training loss  0.506 in Step 1300\n","Training loss  0.568 in Step 1400\n","Training loss  0.668 in Step 1500\n","Training loss  0.567 in Step 1600\n","Training loss  0.566 in Step 1700\n","Training loss  0.621 in Step 1800\n","Training loss  0.729 in Step 1900\n","Training loss  0.662 in Step 2000\n","Training loss  0.465 in Step 2100\n","Training loss  0.620 in Step 2200\n","Training loss  0.549 in Step 2300\n","Training loss  0.612 in Step 2400\n","Training loss  0.574 in Step 2500\n","Training loss  0.495 in Step 2600\n","Training loss  0.528 in Step 2700\n","Training loss  0.555 in Step 2800\n","Training loss  0.804 in Step 2900\n","Training loss  0.408 in Step 3000\n","Training loss  0.275 in Step 3100\n","Training loss  0.559 in Step 3200\n","Training loss  0.651 in Step 3300\n","Training loss  0.526 in Step 3400\n","Training loss  0.594 in Step 3500\n","Training loss  0.635 in Step 3600\n","Training loss  0.603 in Step 3700\n","Training loss  0.700 in Step 3800\n","Training loss  0.499 in Step 3900\n","Training loss  0.505 in Step 4000\n","Training loss  0.472 in Step 4100\n","Training loss  0.606 in Step 4200\n","Training loss  0.452 in Step 4300\n","Training loss  0.397 in Step 4400\n","Training loss  0.551 in Step 4500\n","Training loss  0.578 in Step 4600\n","Training loss  0.461 in Step 4700\n","Training loss  0.598 in Step 4800\n","Training loss  0.673 in Step 4900\n","Training loss  0.392 in Step 5000\n","Training loss  0.367 in Step 5100\n","Training loss  0.433 in Step 5200\n","Training loss  0.438 in Step 5300\n","Training loss  0.429 in Step 5400\n","Training loss  0.492 in Step 5500\n","Training loss  0.531 in Step 5600\n","Training loss  0.615 in Step 5700\n","Training loss  0.504 in Step 5800\n","Training loss  0.601 in Step 5900\n","Training loss  0.587 in Step 6000\n","Training loss  0.522 in Step 6100\n","Training loss  0.773 in Step 6200\n","Training loss  0.541 in Step 6300\n","Training loss  0.659 in Step 6400\n","Training loss  0.647 in Step 6500\n","Training loss  0.574 in Step 6600\n","Training loss  0.468 in Step 6700\n","Training loss  0.437 in Step 6800\n","Training loss  0.427 in Step 6900\n","Training loss  0.553 in Step 7000\n","Training loss  0.459 in Step 7100\n","Training loss  0.561 in Step 7200\n","※※※Training loss  0.544※※※\n","Training timepoint saved\n","Valid loss  0.669 in Step 0\n","Valid loss  0.383 in Step 100\n","Valid loss  0.546 in Step 200\n","Valid loss  0.775 in Step 300\n","Valid loss  0.455 in Step 400\n","Valid loss  0.458 in Step 500\n","Valid loss  0.817 in Step 600\n","Valid loss  0.559 in Step 700\n","Valid loss  0.514 in Step 800\n","Valid loss  0.588 in Step 900\n","Valid loss  0.741 in Step 1000\n","Valid loss  0.429 in Step 1100\n","Valid loss  0.490 in Step 1200\n","Valid loss  0.361 in Step 1300\n","Valid loss  0.232 in Step 1400\n","Valid loss  0.258 in Step 1500\n","Valid loss  0.407 in Step 1600\n","Valid loss  0.543※※※\n","Epoch 17\n","Training loss  0.437 in Step 0\n","Training loss  0.339 in Step 100\n","Training loss  0.776 in Step 200\n","Training loss  0.703 in Step 300\n","Training loss  0.663 in Step 400\n","Training loss  0.742 in Step 500\n","Training loss  0.683 in Step 600\n","Training loss  0.517 in Step 700\n","Training loss  0.485 in Step 800\n","Training loss  0.557 in Step 900\n","Training loss  0.300 in Step 1000\n","Training loss  0.452 in Step 1100\n","Training loss  0.554 in Step 1200\n","Training loss  0.512 in Step 1300\n","Training loss  0.584 in Step 1400\n","Training loss  0.666 in Step 1500\n","Training loss  0.569 in Step 1600\n","Training loss  0.553 in Step 1700\n","Training loss  0.618 in Step 1800\n","Training loss  0.709 in Step 1900\n","Training loss  0.670 in Step 2000\n","Training loss  0.460 in Step 2100\n","Training loss  0.606 in Step 2200\n","Training loss  0.544 in Step 2300\n","Training loss  0.630 in Step 2400\n","Training loss  0.567 in Step 2500\n","Training loss  0.493 in Step 2600\n","Training loss  0.529 in Step 2700\n","Training loss  0.559 in Step 2800\n","Training loss  0.803 in Step 2900\n","Training loss  0.401 in Step 3000\n","Training loss  0.275 in Step 3100\n","Training loss  0.546 in Step 3200\n","Training loss  0.622 in Step 3300\n","Training loss  0.521 in Step 3400\n","Training loss  0.568 in Step 3500\n","Training loss  0.627 in Step 3600\n","Training loss  0.582 in Step 3700\n","Training loss  0.698 in Step 3800\n","Training loss  0.512 in Step 3900\n","Training loss  0.504 in Step 4000\n","Training loss  0.469 in Step 4100\n","Training loss  0.607 in Step 4200\n","Training loss  0.445 in Step 4300\n","Training loss  0.404 in Step 4400\n","Training loss  0.570 in Step 4500\n","Training loss  0.580 in Step 4600\n","Training loss  0.471 in Step 4700\n","Training loss  0.593 in Step 4800\n","Training loss  0.659 in Step 4900\n","Training loss  0.403 in Step 5000\n","Training loss  0.362 in Step 5100\n","Training loss  0.445 in Step 5200\n","Training loss  0.430 in Step 5300\n","Training loss  0.439 in Step 5400\n","Training loss  0.489 in Step 5500\n","Training loss  0.548 in Step 5600\n","Training loss  0.620 in Step 5700\n","Training loss  0.491 in Step 5800\n","Training loss  0.607 in Step 5900\n","Training loss  0.566 in Step 6000\n","Training loss  0.515 in Step 6100\n","Training loss  0.780 in Step 6200\n","Training loss  0.541 in Step 6300\n","Training loss  0.663 in Step 6400\n","Training loss  0.653 in Step 6500\n","Training loss  0.567 in Step 6600\n","Training loss  0.469 in Step 6700\n","Training loss  0.425 in Step 6800\n","Training loss  0.425 in Step 6900\n","Training loss  0.554 in Step 7000\n","Training loss  0.467 in Step 7100\n","Training loss  0.554 in Step 7200\n","※※※Training loss  0.543※※※\n","Training timepoint saved\n","Valid loss  0.658 in Step 0\n","Valid loss  0.387 in Step 100\n","Valid loss  0.538 in Step 200\n","Valid loss  0.742 in Step 300\n","Valid loss  0.460 in Step 400\n","Valid loss  0.462 in Step 500\n","Valid loss  0.802 in Step 600\n","Valid loss  0.560 in Step 700\n","Valid loss  0.526 in Step 800\n","Valid loss  0.593 in Step 900\n","Valid loss  0.735 in Step 1000\n","Valid loss  0.418 in Step 1100\n","Valid loss  0.478 in Step 1200\n","Valid loss  0.355 in Step 1300\n","Valid loss  0.242 in Step 1400\n","Valid loss  0.260 in Step 1500\n","Valid loss  0.410 in Step 1600\n","Valid loss  0.542※※※\n","Epoch 18\n","Training loss  0.443 in Step 0\n","Training loss  0.336 in Step 100\n","Training loss  0.767 in Step 200\n","Training loss  0.722 in Step 300\n","Training loss  0.664 in Step 400\n","Training loss  0.726 in Step 500\n","Training loss  0.688 in Step 600\n","Training loss  0.525 in Step 700\n","Training loss  0.480 in Step 800\n","Training loss  0.558 in Step 900\n","Training loss  0.306 in Step 1000\n","Training loss  0.443 in Step 1100\n","Training loss  0.567 in Step 1200\n","Training loss  0.516 in Step 1300\n","Training loss  0.558 in Step 1400\n","Training loss  0.644 in Step 1500\n","Training loss  0.569 in Step 1600\n","Training loss  0.565 in Step 1700\n","Training loss  0.628 in Step 1800\n","Training loss  0.712 in Step 1900\n","Training loss  0.672 in Step 2000\n","Training loss  0.455 in Step 2100\n","Training loss  0.613 in Step 2200\n","Training loss  0.539 in Step 2300\n","Training loss  0.611 in Step 2400\n","Training loss  0.573 in Step 2500\n","Training loss  0.490 in Step 2600\n","Training loss  0.521 in Step 2700\n","Training loss  0.543 in Step 2800\n","Training loss  0.795 in Step 2900\n","Training loss  0.404 in Step 3000\n","Training loss  0.267 in Step 3100\n","Training loss  0.544 in Step 3200\n","Training loss  0.626 in Step 3300\n","Training loss  0.530 in Step 3400\n","Training loss  0.589 in Step 3500\n","Training loss  0.627 in Step 3600\n","Training loss  0.586 in Step 3700\n","Training loss  0.697 in Step 3800\n","Training loss  0.500 in Step 3900\n","Training loss  0.490 in Step 4000\n","Training loss  0.490 in Step 4100\n","Training loss  0.600 in Step 4200\n","Training loss  0.448 in Step 4300\n","Training loss  0.403 in Step 4400\n","Training loss  0.559 in Step 4500\n","Training loss  0.577 in Step 4600\n","Training loss  0.487 in Step 4700\n","Training loss  0.594 in Step 4800\n","Training loss  0.661 in Step 4900\n","Training loss  0.392 in Step 5000\n","Training loss  0.365 in Step 5100\n","Training loss  0.446 in Step 5200\n","Training loss  0.441 in Step 5300\n","Training loss  0.433 in Step 5400\n","Training loss  0.486 in Step 5500\n","Training loss  0.530 in Step 5600\n","Training loss  0.617 in Step 5700\n","Training loss  0.503 in Step 5800\n","Training loss  0.599 in Step 5900\n","Training loss  0.567 in Step 6000\n","Training loss  0.513 in Step 6100\n","Training loss  0.786 in Step 6200\n","Training loss  0.564 in Step 6300\n","Training loss  0.642 in Step 6400\n","Training loss  0.657 in Step 6500\n","Training loss  0.567 in Step 6600\n","Training loss  0.474 in Step 6700\n","Training loss  0.419 in Step 6800\n","Training loss  0.415 in Step 6900\n","Training loss  0.540 in Step 7000\n","Training loss  0.460 in Step 7100\n","Training loss  0.561 in Step 7200\n","※※※Training loss  0.541※※※\n","Training timepoint saved\n","Valid loss  0.660 in Step 0\n","Valid loss  0.387 in Step 100\n","Valid loss  0.538 in Step 200\n","Valid loss  0.767 in Step 300\n","Valid loss  0.451 in Step 400\n","Valid loss  0.453 in Step 500\n","Valid loss  0.805 in Step 600\n","Valid loss  0.542 in Step 700\n","Valid loss  0.521 in Step 800\n","Valid loss  0.588 in Step 900\n","Valid loss  0.737 in Step 1000\n","Valid loss  0.423 in Step 1100\n","Valid loss  0.484 in Step 1200\n","Valid loss  0.355 in Step 1300\n","Valid loss  0.240 in Step 1400\n","Valid loss  0.259 in Step 1500\n","Valid loss  0.407 in Step 1600\n","Valid loss  0.540※※※\n","Epoch 19\n","Training loss  0.435 in Step 0\n","Training loss  0.331 in Step 100\n","Training loss  0.764 in Step 200\n","Training loss  0.719 in Step 300\n","Training loss  0.659 in Step 400\n","Training loss  0.733 in Step 500\n","Training loss  0.681 in Step 600\n","Training loss  0.523 in Step 700\n","Training loss  0.476 in Step 800\n","Training loss  0.560 in Step 900\n","Training loss  0.298 in Step 1000\n","Training loss  0.442 in Step 1100\n","Training loss  0.563 in Step 1200\n","Training loss  0.521 in Step 1300\n","Training loss  0.566 in Step 1400\n","Training loss  0.654 in Step 1500\n","Training loss  0.571 in Step 1600\n","Training loss  0.559 in Step 1700\n","Training loss  0.621 in Step 1800\n","Training loss  0.720 in Step 1900\n","Training loss  0.677 in Step 2000\n","Training loss  0.460 in Step 2100\n","Training loss  0.618 in Step 2200\n","Training loss  0.549 in Step 2300\n","Training loss  0.606 in Step 2400\n","Training loss  0.549 in Step 2500\n","Training loss  0.505 in Step 2600\n","Training loss  0.518 in Step 2700\n","Training loss  0.549 in Step 2800\n","Training loss  0.795 in Step 2900\n","Training loss  0.409 in Step 3000\n","Training loss  0.276 in Step 3100\n","Training loss  0.547 in Step 3200\n","Training loss  0.630 in Step 3300\n","Training loss  0.530 in Step 3400\n","Training loss  0.566 in Step 3500\n","Training loss  0.624 in Step 3600\n","Training loss  0.595 in Step 3700\n","Training loss  0.685 in Step 3800\n","Training loss  0.497 in Step 3900\n","Training loss  0.509 in Step 4000\n","Training loss  0.464 in Step 4100\n","Training loss  0.611 in Step 4200\n","Training loss  0.444 in Step 4300\n","Training loss  0.414 in Step 4400\n","Training loss  0.552 in Step 4500\n","Training loss  0.571 in Step 4600\n","Training loss  0.482 in Step 4700\n","Training loss  0.588 in Step 4800\n","Training loss  0.658 in Step 4900\n","Training loss  0.397 in Step 5000\n","Training loss  0.366 in Step 5100\n","Training loss  0.432 in Step 5200\n","Training loss  0.435 in Step 5300\n","Training loss  0.432 in Step 5400\n","Training loss  0.485 in Step 5500\n","Training loss  0.529 in Step 5600\n","Training loss  0.620 in Step 5700\n","Training loss  0.496 in Step 5800\n","Training loss  0.601 in Step 5900\n","Training loss  0.555 in Step 6000\n","Training loss  0.507 in Step 6100\n","Training loss  0.782 in Step 6200\n","Training loss  0.539 in Step 6300\n","Training loss  0.657 in Step 6400\n","Training loss  0.648 in Step 6500\n","Training loss  0.566 in Step 6600\n","Training loss  0.477 in Step 6700\n","Training loss  0.427 in Step 6800\n","Training loss  0.411 in Step 6900\n","Training loss  0.548 in Step 7000\n","Training loss  0.452 in Step 7100\n","Training loss  0.547 in Step 7200\n","※※※Training loss  0.540※※※\n","Training timepoint saved\n","Valid loss  0.660 in Step 0\n","Valid loss  0.391 in Step 100\n","Valid loss  0.540 in Step 200\n","Valid loss  0.785 in Step 300\n","Valid loss  0.452 in Step 400\n","Valid loss  0.449 in Step 500\n","Valid loss  0.818 in Step 600\n","Valid loss  0.544 in Step 700\n","Valid loss  0.525 in Step 800\n","Valid loss  0.582 in Step 900\n","Valid loss  0.733 in Step 1000\n","Valid loss  0.443 in Step 1100\n","Valid loss  0.486 in Step 1200\n","Valid loss  0.382 in Step 1300\n","Valid loss  0.240 in Step 1400\n","Valid loss  0.256 in Step 1500\n","Valid loss  0.409 in Step 1600\n","Valid loss  0.541※※※\n","Epoch 20\n","Training loss  0.426 in Step 0\n","Training loss  0.339 in Step 100\n","Training loss  0.757 in Step 200\n","Training loss  0.715 in Step 300\n","Training loss  0.658 in Step 400\n","Training loss  0.730 in Step 500\n","Training loss  0.685 in Step 600\n","Training loss  0.525 in Step 700\n","Training loss  0.482 in Step 800\n","Training loss  0.555 in Step 900\n","Training loss  0.294 in Step 1000\n","Training loss  0.449 in Step 1100\n","Training loss  0.560 in Step 1200\n","Training loss  0.509 in Step 1300\n","Training loss  0.558 in Step 1400\n","Training loss  0.653 in Step 1500\n","Training loss  0.573 in Step 1600\n","Training loss  0.565 in Step 1700\n","Training loss  0.612 in Step 1800\n","Training loss  0.698 in Step 1900\n","Training loss  0.658 in Step 2000\n","Training loss  0.448 in Step 2100\n","Training loss  0.602 in Step 2200\n","Training loss  0.549 in Step 2300\n","Training loss  0.609 in Step 2400\n","Training loss  0.566 in Step 2500\n","Training loss  0.498 in Step 2600\n","Training loss  0.519 in Step 2700\n","Training loss  0.547 in Step 2800\n","Training loss  0.792 in Step 2900\n","Training loss  0.403 in Step 3000\n","Training loss  0.273 in Step 3100\n","Training loss  0.557 in Step 3200\n","Training loss  0.632 in Step 3300\n","Training loss  0.521 in Step 3400\n","Training loss  0.583 in Step 3500\n","Training loss  0.615 in Step 3600\n","Training loss  0.603 in Step 3700\n","Training loss  0.684 in Step 3800\n","Training loss  0.497 in Step 3900\n","Training loss  0.494 in Step 4000\n","Training loss  0.475 in Step 4100\n","Training loss  0.598 in Step 4200\n","Training loss  0.447 in Step 4300\n","Training loss  0.406 in Step 4400\n","Training loss  0.546 in Step 4500\n","Training loss  0.577 in Step 4600\n","Training loss  0.464 in Step 4700\n","Training loss  0.585 in Step 4800\n","Training loss  0.668 in Step 4900\n","Training loss  0.390 in Step 5000\n","Training loss  0.364 in Step 5100\n","Training loss  0.437 in Step 5200\n","Training loss  0.433 in Step 5300\n","Training loss  0.424 in Step 5400\n","Training loss  0.478 in Step 5500\n","Training loss  0.534 in Step 5600\n","Training loss  0.610 in Step 5700\n","Training loss  0.500 in Step 5800\n","Training loss  0.598 in Step 5900\n","Training loss  0.569 in Step 6000\n","Training loss  0.511 in Step 6100\n","Training loss  0.764 in Step 6200\n","Training loss  0.538 in Step 6300\n","Training loss  0.654 in Step 6400\n","Training loss  0.658 in Step 6500\n","Training loss  0.576 in Step 6600\n","Training loss  0.471 in Step 6700\n","Training loss  0.424 in Step 6800\n","Training loss  0.412 in Step 6900\n","Training loss  0.532 in Step 7000\n","Training loss  0.456 in Step 7100\n","Training loss  0.552 in Step 7200\n","※※※Training loss  0.538※※※\n","Training timepoint saved\n","Valid loss  0.654 in Step 0\n","Valid loss  0.386 in Step 100\n","Valid loss  0.549 in Step 200\n","Valid loss  0.766 in Step 300\n","Valid loss  0.454 in Step 400\n","Valid loss  0.451 in Step 500\n","Valid loss  0.815 in Step 600\n","Valid loss  0.541 in Step 700\n","Valid loss  0.520 in Step 800\n","Valid loss  0.594 in Step 900\n","Valid loss  0.727 in Step 1000\n","Valid loss  0.435 in Step 1100\n","Valid loss  0.473 in Step 1200\n","Valid loss  0.357 in Step 1300\n","Valid loss  0.235 in Step 1400\n","Valid loss  0.261 in Step 1500\n","Valid loss  0.410 in Step 1600\n","Valid loss  0.540※※※\n","Epoch 21\n","Training loss  0.421 in Step 0\n","Training loss  0.333 in Step 100\n","Training loss  0.738 in Step 200\n","Training loss  0.701 in Step 300\n","Training loss  0.656 in Step 400\n","Training loss  0.710 in Step 500\n","Training loss  0.689 in Step 600\n","Training loss  0.522 in Step 700\n","Training loss  0.478 in Step 800\n","Training loss  0.565 in Step 900\n","Training loss  0.298 in Step 1000\n","Training loss  0.445 in Step 1100\n","Training loss  0.560 in Step 1200\n","Training loss  0.509 in Step 1300\n","Training loss  0.557 in Step 1400\n","Training loss  0.646 in Step 1500\n","Training loss  0.576 in Step 1600\n","Training loss  0.563 in Step 1700\n","Training loss  0.609 in Step 1800\n","Training loss  0.699 in Step 1900\n","Training loss  0.659 in Step 2000\n","Training loss  0.454 in Step 2100\n","Training loss  0.615 in Step 2200\n","Training loss  0.545 in Step 2300\n","Training loss  0.608 in Step 2400\n","Training loss  0.559 in Step 2500\n","Training loss  0.491 in Step 2600\n","Training loss  0.515 in Step 2700\n","Training loss  0.546 in Step 2800\n","Training loss  0.784 in Step 2900\n","Training loss  0.399 in Step 3000\n","Training loss  0.273 in Step 3100\n","Training loss  0.549 in Step 3200\n","Training loss  0.622 in Step 3300\n","Training loss  0.535 in Step 3400\n","Training loss  0.585 in Step 3500\n","Training loss  0.630 in Step 3600\n","Training loss  0.590 in Step 3700\n","Training loss  0.671 in Step 3800\n","Training loss  0.499 in Step 3900\n","Training loss  0.497 in Step 4000\n","Training loss  0.461 in Step 4100\n","Training loss  0.597 in Step 4200\n","Training loss  0.444 in Step 4300\n","Training loss  0.403 in Step 4400\n","Training loss  0.545 in Step 4500\n","Training loss  0.568 in Step 4600\n","Training loss  0.466 in Step 4700\n","Training loss  0.580 in Step 4800\n","Training loss  0.653 in Step 4900\n","Training loss  0.410 in Step 5000\n","Training loss  0.363 in Step 5100\n","Training loss  0.437 in Step 5200\n","Training loss  0.438 in Step 5300\n","Training loss  0.425 in Step 5400\n","Training loss  0.476 in Step 5500\n","Training loss  0.539 in Step 5600\n","Training loss  0.610 in Step 5700\n","Training loss  0.484 in Step 5800\n","Training loss  0.604 in Step 5900\n","Training loss  0.562 in Step 6000\n","Training loss  0.511 in Step 6100\n","Training loss  0.779 in Step 6200\n","Training loss  0.547 in Step 6300\n","Training loss  0.647 in Step 6400\n","Training loss  0.651 in Step 6500\n","Training loss  0.570 in Step 6600\n","Training loss  0.473 in Step 6700\n","Training loss  0.421 in Step 6800\n","Training loss  0.417 in Step 6900\n","Training loss  0.536 in Step 7000\n","Training loss  0.460 in Step 7100\n","Training loss  0.554 in Step 7200\n","※※※Training loss  0.537※※※\n","Training timepoint saved\n","Valid loss  0.608 in Step 0\n","Valid loss  0.380 in Step 100\n","Valid loss  0.533 in Step 200\n","Valid loss  0.751 in Step 300\n","Valid loss  0.454 in Step 400\n","Valid loss  0.463 in Step 500\n","Valid loss  0.801 in Step 600\n","Valid loss  0.547 in Step 700\n","Valid loss  0.508 in Step 800\n","Valid loss  0.576 in Step 900\n","Valid loss  0.742 in Step 1000\n","Valid loss  0.428 in Step 1100\n","Valid loss  0.474 in Step 1200\n","Valid loss  0.353 in Step 1300\n","Valid loss  0.237 in Step 1400\n","Valid loss  0.252 in Step 1500\n","Valid loss  0.410 in Step 1600\n","Valid loss  0.538※※※\n","Epoch 22\n","Training loss  0.414 in Step 0\n","Training loss  0.328 in Step 100\n","Training loss  0.738 in Step 200\n","Training loss  0.710 in Step 300\n","Training loss  0.650 in Step 400\n","Training loss  0.733 in Step 500\n","Training loss  0.682 in Step 600\n","Training loss  0.518 in Step 700\n","Training loss  0.478 in Step 800\n","Training loss  0.563 in Step 900\n","Training loss  0.297 in Step 1000\n","Training loss  0.445 in Step 1100\n","Training loss  0.558 in Step 1200\n","Training loss  0.501 in Step 1300\n","Training loss  0.558 in Step 1400\n","Training loss  0.652 in Step 1500\n","Training loss  0.574 in Step 1600\n","Training loss  0.562 in Step 1700\n","Training loss  0.606 in Step 1800\n","Training loss  0.708 in Step 1900\n","Training loss  0.647 in Step 2000\n","Training loss  0.449 in Step 2100\n","Training loss  0.613 in Step 2200\n","Training loss  0.536 in Step 2300\n","Training loss  0.604 in Step 2400\n","Training loss  0.564 in Step 2500\n","Training loss  0.490 in Step 2600\n","Training loss  0.516 in Step 2700\n","Training loss  0.546 in Step 2800\n","Training loss  0.779 in Step 2900\n","Training loss  0.399 in Step 3000\n","Training loss  0.271 in Step 3100\n","Training loss  0.550 in Step 3200\n","Training loss  0.639 in Step 3300\n","Training loss  0.525 in Step 3400\n","Training loss  0.612 in Step 3500\n","Training loss  0.623 in Step 3600\n","Training loss  0.587 in Step 3700\n","Training loss  0.661 in Step 3800\n","Training loss  0.495 in Step 3900\n","Training loss  0.498 in Step 4000\n","Training loss  0.472 in Step 4100\n","Training loss  0.600 in Step 4200\n","Training loss  0.443 in Step 4300\n","Training loss  0.400 in Step 4400\n","Training loss  0.554 in Step 4500\n","Training loss  0.575 in Step 4600\n","Training loss  0.463 in Step 4700\n","Training loss  0.594 in Step 4800\n","Training loss  0.669 in Step 4900\n","Training loss  0.387 in Step 5000\n","Training loss  0.355 in Step 5100\n","Training loss  0.433 in Step 5200\n","Training loss  0.436 in Step 5300\n","Training loss  0.434 in Step 5400\n","Training loss  0.483 in Step 5500\n","Training loss  0.532 in Step 5600\n","Training loss  0.607 in Step 5700\n","Training loss  0.482 in Step 5800\n","Training loss  0.585 in Step 5900\n","Training loss  0.562 in Step 6000\n","Training loss  0.506 in Step 6100\n","Training loss  0.759 in Step 6200\n","Training loss  0.541 in Step 6300\n","Training loss  0.632 in Step 6400\n","Training loss  0.658 in Step 6500\n","Training loss  0.567 in Step 6600\n","Training loss  0.464 in Step 6700\n","Training loss  0.424 in Step 6800\n","Training loss  0.411 in Step 6900\n","Training loss  0.544 in Step 7000\n","Training loss  0.458 in Step 7100\n","Training loss  0.550 in Step 7200\n","※※※Training loss  0.535※※※\n","Training timepoint saved\n","Valid loss  0.647 in Step 0\n","Valid loss  0.386 in Step 100\n","Valid loss  0.543 in Step 200\n","Valid loss  0.740 in Step 300\n","Valid loss  0.457 in Step 400\n","Valid loss  0.435 in Step 500\n","Valid loss  0.801 in Step 600\n","Valid loss  0.531 in Step 700\n","Valid loss  0.509 in Step 800\n","Valid loss  0.578 in Step 900\n","Valid loss  0.725 in Step 1000\n","Valid loss  0.406 in Step 1100\n","Valid loss  0.474 in Step 1200\n","Valid loss  0.362 in Step 1300\n","Valid loss  0.236 in Step 1400\n","Valid loss  0.255 in Step 1500\n","Valid loss  0.415 in Step 1600\n","Valid loss  0.537※※※\n","Epoch 23\n","Training loss  0.421 in Step 0\n","Training loss  0.336 in Step 100\n","Training loss  0.741 in Step 200\n","Training loss  0.687 in Step 300\n","Training loss  0.651 in Step 400\n","Training loss  0.710 in Step 500\n","Training loss  0.688 in Step 600\n","Training loss  0.514 in Step 700\n","Training loss  0.474 in Step 800\n","Training loss  0.547 in Step 900\n","Training loss  0.286 in Step 1000\n","Training loss  0.456 in Step 1100\n","Training loss  0.558 in Step 1200\n","Training loss  0.509 in Step 1300\n","Training loss  0.567 in Step 1400\n","Training loss  0.644 in Step 1500\n","Training loss  0.568 in Step 1600\n","Training loss  0.565 in Step 1700\n","Training loss  0.614 in Step 1800\n","Training loss  0.706 in Step 1900\n","Training loss  0.657 in Step 2000\n","Training loss  0.446 in Step 2100\n","Training loss  0.606 in Step 2200\n","Training loss  0.542 in Step 2300\n","Training loss  0.606 in Step 2400\n","Training loss  0.556 in Step 2500\n","Training loss  0.484 in Step 2600\n","Training loss  0.521 in Step 2700\n","Training loss  0.537 in Step 2800\n","Training loss  0.777 in Step 2900\n","Training loss  0.393 in Step 3000\n","Training loss  0.279 in Step 3100\n","Training loss  0.561 in Step 3200\n","Training loss  0.619 in Step 3300\n","Training loss  0.536 in Step 3400\n","Training loss  0.589 in Step 3500\n","Training loss  0.625 in Step 3600\n","Training loss  0.589 in Step 3700\n","Training loss  0.687 in Step 3800\n","Training loss  0.494 in Step 3900\n","Training loss  0.496 in Step 4000\n","Training loss  0.469 in Step 4100\n","Training loss  0.594 in Step 4200\n","Training loss  0.443 in Step 4300\n","Training loss  0.397 in Step 4400\n","Training loss  0.553 in Step 4500\n","Training loss  0.575 in Step 4600\n","Training loss  0.461 in Step 4700\n","Training loss  0.591 in Step 4800\n","Training loss  0.656 in Step 4900\n","Training loss  0.396 in Step 5000\n","Training loss  0.361 in Step 5100\n","Training loss  0.433 in Step 5200\n","Training loss  0.436 in Step 5300\n","Training loss  0.427 in Step 5400\n","Training loss  0.483 in Step 5500\n","Training loss  0.529 in Step 5600\n","Training loss  0.594 in Step 5700\n","Training loss  0.485 in Step 5800\n","Training loss  0.591 in Step 5900\n","Training loss  0.565 in Step 6000\n","Training loss  0.515 in Step 6100\n","Training loss  0.762 in Step 6200\n","Training loss  0.533 in Step 6300\n","Training loss  0.639 in Step 6400\n","Training loss  0.653 in Step 6500\n","Training loss  0.565 in Step 6600\n","Training loss  0.470 in Step 6700\n","Training loss  0.422 in Step 6800\n","Training loss  0.412 in Step 6900\n","Training loss  0.534 in Step 7000\n","Training loss  0.467 in Step 7100\n","Training loss  0.550 in Step 7200\n","※※※Training loss  0.535※※※\n","Training timepoint saved\n","Valid loss  0.626 in Step 0\n","Valid loss  0.376 in Step 100\n","Valid loss  0.543 in Step 200\n","Valid loss  0.782 in Step 300\n","Valid loss  0.456 in Step 400\n","Valid loss  0.446 in Step 500\n","Valid loss  0.799 in Step 600\n","Valid loss  0.541 in Step 700\n","Valid loss  0.507 in Step 800\n","Valid loss  0.572 in Step 900\n","Valid loss  0.731 in Step 1000\n","Valid loss  0.407 in Step 1100\n","Valid loss  0.475 in Step 1200\n","Valid loss  0.355 in Step 1300\n","Valid loss  0.232 in Step 1400\n","Valid loss  0.254 in Step 1500\n","Valid loss  0.414 in Step 1600\n","Valid loss  0.535※※※\n","Epoch 24\n","Training loss  0.426 in Step 0\n","Training loss  0.334 in Step 100\n","Training loss  0.745 in Step 200\n","Training loss  0.691 in Step 300\n","Training loss  0.664 in Step 400\n","Training loss  0.719 in Step 500\n","Training loss  0.690 in Step 600\n","Training loss  0.528 in Step 700\n","Training loss  0.473 in Step 800\n","Training loss  0.554 in Step 900\n","Training loss  0.286 in Step 1000\n","Training loss  0.442 in Step 1100\n","Training loss  0.553 in Step 1200\n","Training loss  0.504 in Step 1300\n","Training loss  0.558 in Step 1400\n","Training loss  0.637 in Step 1500\n","Training loss  0.571 in Step 1600\n","Training loss  0.565 in Step 1700\n","Training loss  0.606 in Step 1800\n","Training loss  0.706 in Step 1900\n","Training loss  0.639 in Step 2000\n","Training loss  0.453 in Step 2100\n","Training loss  0.602 in Step 2200\n","Training loss  0.537 in Step 2300\n","Training loss  0.601 in Step 2400\n","Training loss  0.567 in Step 2500\n","Training loss  0.487 in Step 2600\n","Training loss  0.518 in Step 2700\n","Training loss  0.530 in Step 2800\n","Training loss  0.788 in Step 2900\n","Training loss  0.405 in Step 3000\n","Training loss  0.264 in Step 3100\n","Training loss  0.567 in Step 3200\n","Training loss  0.622 in Step 3300\n","Training loss  0.511 in Step 3400\n","Training loss  0.583 in Step 3500\n","Training loss  0.618 in Step 3600\n","Training loss  0.604 in Step 3700\n","Training loss  0.657 in Step 3800\n","Training loss  0.497 in Step 3900\n","Training loss  0.498 in Step 4000\n","Training loss  0.463 in Step 4100\n","Training loss  0.594 in Step 4200\n","Training loss  0.442 in Step 4300\n","Training loss  0.398 in Step 4400\n","Training loss  0.542 in Step 4500\n","Training loss  0.580 in Step 4600\n","Training loss  0.463 in Step 4700\n","Training loss  0.583 in Step 4800\n","Training loss  0.659 in Step 4900\n","Training loss  0.395 in Step 5000\n","Training loss  0.355 in Step 5100\n","Training loss  0.424 in Step 5200\n","Training loss  0.436 in Step 5300\n","Training loss  0.426 in Step 5400\n","Training loss  0.476 in Step 5500\n","Training loss  0.526 in Step 5600\n","Training loss  0.613 in Step 5700\n","Training loss  0.486 in Step 5800\n","Training loss  0.582 in Step 5900\n","Training loss  0.567 in Step 6000\n","Training loss  0.499 in Step 6100\n","Training loss  0.750 in Step 6200\n","Training loss  0.539 in Step 6300\n","Training loss  0.647 in Step 6400\n","Training loss  0.655 in Step 6500\n","Training loss  0.552 in Step 6600\n","Training loss  0.481 in Step 6700\n","Training loss  0.423 in Step 6800\n","Training loss  0.405 in Step 6900\n","Training loss  0.531 in Step 7000\n","Training loss  0.457 in Step 7100\n","Training loss  0.550 in Step 7200\n","※※※Training loss  0.534※※※\n","Training timepoint saved\n","Valid loss  0.619 in Step 0\n","Valid loss  0.376 in Step 100\n","Valid loss  0.536 in Step 200\n","Valid loss  0.758 in Step 300\n","Valid loss  0.447 in Step 400\n","Valid loss  0.435 in Step 500\n","Valid loss  0.797 in Step 600\n","Valid loss  0.547 in Step 700\n","Valid loss  0.519 in Step 800\n","Valid loss  0.579 in Step 900\n","Valid loss  0.733 in Step 1000\n","Valid loss  0.412 in Step 1100\n","Valid loss  0.478 in Step 1200\n","Valid loss  0.351 in Step 1300\n","Valid loss  0.230 in Step 1400\n","Valid loss  0.249 in Step 1500\n","Valid loss  0.410 in Step 1600\n","Valid loss  0.535※※※\n","Epoch 25\n","Training loss  0.411 in Step 0\n","Training loss  0.332 in Step 100\n","Training loss  0.750 in Step 200\n","Training loss  0.700 in Step 300\n","Training loss  0.651 in Step 400\n","Training loss  0.729 in Step 500\n","Training loss  0.687 in Step 600\n","Training loss  0.517 in Step 700\n","Training loss  0.466 in Step 800\n","Training loss  0.561 in Step 900\n","Training loss  0.287 in Step 1000\n","Training loss  0.444 in Step 1100\n","Training loss  0.554 in Step 1200\n","Training loss  0.503 in Step 1300\n","Training loss  0.566 in Step 1400\n","Training loss  0.641 in Step 1500\n","Training loss  0.554 in Step 1600\n","Training loss  0.573 in Step 1700\n","Training loss  0.613 in Step 1800\n","Training loss  0.708 in Step 1900\n","Training loss  0.654 in Step 2000\n","Training loss  0.455 in Step 2100\n","Training loss  0.611 in Step 2200\n","Training loss  0.540 in Step 2300\n","Training loss  0.601 in Step 2400\n","Training loss  0.542 in Step 2500\n","Training loss  0.481 in Step 2600\n","Training loss  0.519 in Step 2700\n","Training loss  0.525 in Step 2800\n","Training loss  0.790 in Step 2900\n","Training loss  0.395 in Step 3000\n","Training loss  0.267 in Step 3100\n","Training loss  0.554 in Step 3200\n","Training loss  0.624 in Step 3300\n","Training loss  0.519 in Step 3400\n","Training loss  0.562 in Step 3500\n","Training loss  0.613 in Step 3600\n","Training loss  0.601 in Step 3700\n","Training loss  0.664 in Step 3800\n","Training loss  0.491 in Step 3900\n","Training loss  0.496 in Step 4000\n","Training loss  0.462 in Step 4100\n","Training loss  0.604 in Step 4200\n","Training loss  0.435 in Step 4300\n","Training loss  0.406 in Step 4400\n","Training loss  0.547 in Step 4500\n","Training loss  0.575 in Step 4600\n","Training loss  0.450 in Step 4700\n","Training loss  0.582 in Step 4800\n","Training loss  0.649 in Step 4900\n","Training loss  0.398 in Step 5000\n","Training loss  0.360 in Step 5100\n","Training loss  0.427 in Step 5200\n","Training loss  0.434 in Step 5300\n","Training loss  0.412 in Step 5400\n","Training loss  0.485 in Step 5500\n","Training loss  0.536 in Step 5600\n","Training loss  0.605 in Step 5700\n","Training loss  0.479 in Step 5800\n","Training loss  0.595 in Step 5900\n","Training loss  0.556 in Step 6000\n","Training loss  0.503 in Step 6100\n","Training loss  0.752 in Step 6200\n","Training loss  0.526 in Step 6300\n","Training loss  0.629 in Step 6400\n","Training loss  0.653 in Step 6500\n","Training loss  0.553 in Step 6600\n","Training loss  0.456 in Step 6700\n","Training loss  0.426 in Step 6800\n","Training loss  0.412 in Step 6900\n","Training loss  0.537 in Step 7000\n","Training loss  0.463 in Step 7100\n","Training loss  0.543 in Step 7200\n","※※※Training loss  0.533※※※\n","Training timepoint saved\n","Valid loss  0.600 in Step 0\n","Valid loss  0.373 in Step 100\n","Valid loss  0.538 in Step 200\n","Valid loss  0.771 in Step 300\n","Valid loss  0.456 in Step 400\n","Valid loss  0.441 in Step 500\n","Valid loss  0.804 in Step 600\n","Valid loss  0.543 in Step 700\n","Valid loss  0.518 in Step 800\n","Valid loss  0.578 in Step 900\n","Valid loss  0.718 in Step 1000\n","Valid loss  0.415 in Step 1100\n","Valid loss  0.476 in Step 1200\n","Valid loss  0.380 in Step 1300\n","Valid loss  0.235 in Step 1400\n","Valid loss  0.261 in Step 1500\n","Valid loss  0.407 in Step 1600\n","Valid loss  0.534※※※\n","Epoch 26\n","Training loss  0.404 in Step 0\n","Training loss  0.328 in Step 100\n","Training loss  0.738 in Step 200\n","Training loss  0.691 in Step 300\n","Training loss  0.643 in Step 400\n","Training loss  0.715 in Step 500\n","Training loss  0.693 in Step 600\n","Training loss  0.520 in Step 700\n","Training loss  0.472 in Step 800\n","Training loss  0.554 in Step 900\n","Training loss  0.285 in Step 1000\n","Training loss  0.448 in Step 1100\n","Training loss  0.549 in Step 1200\n","Training loss  0.512 in Step 1300\n","Training loss  0.556 in Step 1400\n","Training loss  0.637 in Step 1500\n","Training loss  0.568 in Step 1600\n","Training loss  0.570 in Step 1700\n","Training loss  0.603 in Step 1800\n","Training loss  0.702 in Step 1900\n","Training loss  0.637 in Step 2000\n","Training loss  0.445 in Step 2100\n","Training loss  0.602 in Step 2200\n","Training loss  0.531 in Step 2300\n","Training loss  0.599 in Step 2400\n","Training loss  0.554 in Step 2500\n","Training loss  0.476 in Step 2600\n","Training loss  0.521 in Step 2700\n","Training loss  0.533 in Step 2800\n","Training loss  0.775 in Step 2900\n","Training loss  0.394 in Step 3000\n","Training loss  0.266 in Step 3100\n","Training loss  0.549 in Step 3200\n","Training loss  0.617 in Step 3300\n","Training loss  0.529 in Step 3400\n","Training loss  0.593 in Step 3500\n","Training loss  0.616 in Step 3600\n","Training loss  0.592 in Step 3700\n","Training loss  0.653 in Step 3800\n","Training loss  0.487 in Step 3900\n","Training loss  0.500 in Step 4000\n","Training loss  0.464 in Step 4100\n","Training loss  0.602 in Step 4200\n","Training loss  0.440 in Step 4300\n","Training loss  0.396 in Step 4400\n","Training loss  0.540 in Step 4500\n","Training loss  0.577 in Step 4600\n","Training loss  0.446 in Step 4700\n","Training loss  0.584 in Step 4800\n","Training loss  0.647 in Step 4900\n","Training loss  0.389 in Step 5000\n","Training loss  0.352 in Step 5100\n","Training loss  0.427 in Step 5200\n","Training loss  0.430 in Step 5300\n","Training loss  0.418 in Step 5400\n","Training loss  0.478 in Step 5500\n","Training loss  0.531 in Step 5600\n","Training loss  0.598 in Step 5700\n","Training loss  0.480 in Step 5800\n","Training loss  0.584 in Step 5900\n","Training loss  0.553 in Step 6000\n","Training loss  0.500 in Step 6100\n","Training loss  0.761 in Step 6200\n","Training loss  0.544 in Step 6300\n","Training loss  0.638 in Step 6400\n","Training loss  0.645 in Step 6500\n","Training loss  0.563 in Step 6600\n","Training loss  0.454 in Step 6700\n","Training loss  0.419 in Step 6800\n","Training loss  0.417 in Step 6900\n","Training loss  0.535 in Step 7000\n","Training loss  0.452 in Step 7100\n","Training loss  0.553 in Step 7200\n","※※※Training loss  0.531※※※\n","Training timepoint saved\n","Valid loss  0.613 in Step 0\n","Valid loss  0.377 in Step 100\n","Valid loss  0.535 in Step 200\n","Valid loss  0.737 in Step 300\n","Valid loss  0.467 in Step 400\n","Valid loss  0.444 in Step 500\n","Valid loss  0.806 in Step 600\n","Valid loss  0.550 in Step 700\n","Valid loss  0.503 in Step 800\n","Valid loss  0.581 in Step 900\n","Valid loss  0.720 in Step 1000\n","Valid loss  0.427 in Step 1100\n","Valid loss  0.480 in Step 1200\n","Valid loss  0.361 in Step 1300\n","Valid loss  0.232 in Step 1400\n","Valid loss  0.256 in Step 1500\n","Valid loss  0.406 in Step 1600\n","Valid loss  0.534※※※\n","Epoch 27\n","Training loss  0.415 in Step 0\n","Training loss  0.332 in Step 100\n","Training loss  0.714 in Step 200\n","Training loss  0.694 in Step 300\n","Training loss  0.646 in Step 400\n","Training loss  0.716 in Step 500\n","Training loss  0.689 in Step 600\n","Training loss  0.522 in Step 700\n","Training loss  0.469 in Step 800\n","Training loss  0.552 in Step 900\n","Training loss  0.293 in Step 1000\n","Training loss  0.435 in Step 1100\n","Training loss  0.544 in Step 1200\n","Training loss  0.498 in Step 1300\n","Training loss  0.557 in Step 1400\n","Training loss  0.637 in Step 1500\n","Training loss  0.555 in Step 1600\n","Training loss  0.569 in Step 1700\n","Training loss  0.610 in Step 1800\n","Training loss  0.712 in Step 1900\n","Training loss  0.634 in Step 2000\n","Training loss  0.447 in Step 2100\n","Training loss  0.618 in Step 2200\n","Training loss  0.537 in Step 2300\n","Training loss  0.609 in Step 2400\n","Training loss  0.543 in Step 2500\n","Training loss  0.480 in Step 2600\n","Training loss  0.515 in Step 2700\n","Training loss  0.539 in Step 2800\n","Training loss  0.771 in Step 2900\n","Training loss  0.400 in Step 3000\n","Training loss  0.269 in Step 3100\n","Training loss  0.553 in Step 3200\n","Training loss  0.606 in Step 3300\n","Training loss  0.526 in Step 3400\n","Training loss  0.524 in Step 3500\n","Training loss  0.612 in Step 3600\n","Training loss  0.592 in Step 3700\n","Training loss  0.651 in Step 3800\n","Training loss  0.483 in Step 3900\n","Training loss  0.478 in Step 4000\n","Training loss  0.454 in Step 4100\n","Training loss  0.600 in Step 4200\n","Training loss  0.440 in Step 4300\n","Training loss  0.400 in Step 4400\n","Training loss  0.557 in Step 4500\n","Training loss  0.571 in Step 4600\n","Training loss  0.448 in Step 4700\n","Training loss  0.594 in Step 4800\n","Training loss  0.647 in Step 4900\n","Training loss  0.395 in Step 5000\n","Training loss  0.364 in Step 5100\n","Training loss  0.430 in Step 5200\n","Training loss  0.428 in Step 5300\n","Training loss  0.414 in Step 5400\n","Training loss  0.479 in Step 5500\n","Training loss  0.537 in Step 5600\n","Training loss  0.608 in Step 5700\n","Training loss  0.473 in Step 5800\n","Training loss  0.584 in Step 5900\n","Training loss  0.556 in Step 6000\n","Training loss  0.509 in Step 6100\n","Training loss  0.750 in Step 6200\n","Training loss  0.542 in Step 6300\n","Training loss  0.630 in Step 6400\n","Training loss  0.655 in Step 6500\n","Training loss  0.563 in Step 6600\n","Training loss  0.466 in Step 6700\n","Training loss  0.421 in Step 6800\n","Training loss  0.411 in Step 6900\n","Training loss  0.536 in Step 7000\n","Training loss  0.454 in Step 7100\n","Training loss  0.545 in Step 7200\n","※※※Training loss  0.530※※※\n","Training timepoint saved\n","Valid loss  0.610 in Step 0\n","Valid loss  0.376 in Step 100\n","Valid loss  0.537 in Step 200\n","Valid loss  0.746 in Step 300\n","Valid loss  0.449 in Step 400\n","Valid loss  0.432 in Step 500\n","Valid loss  0.792 in Step 600\n","Valid loss  0.537 in Step 700\n","Valid loss  0.516 in Step 800\n","Valid loss  0.578 in Step 900\n","Valid loss  0.724 in Step 1000\n","Valid loss  0.409 in Step 1100\n","Valid loss  0.477 in Step 1200\n","Valid loss  0.363 in Step 1300\n","Valid loss  0.233 in Step 1400\n","Valid loss  0.254 in Step 1500\n","Valid loss  0.407 in Step 1600\n","Valid loss  0.532※※※\n","Epoch 28\n","Training loss  0.409 in Step 0\n","Training loss  0.332 in Step 100\n","Training loss  0.706 in Step 200\n","Training loss  0.701 in Step 300\n","Training loss  0.650 in Step 400\n","Training loss  0.722 in Step 500\n","Training loss  0.685 in Step 600\n","Training loss  0.517 in Step 700\n","Training loss  0.467 in Step 800\n","Training loss  0.548 in Step 900\n","Training loss  0.294 in Step 1000\n","Training loss  0.449 in Step 1100\n","Training loss  0.543 in Step 1200\n","Training loss  0.507 in Step 1300\n","Training loss  0.559 in Step 1400\n","Training loss  0.641 in Step 1500\n","Training loss  0.566 in Step 1600\n","Training loss  0.560 in Step 1700\n","Training loss  0.612 in Step 1800\n","Training loss  0.695 in Step 1900\n","Training loss  0.640 in Step 2000\n","Training loss  0.457 in Step 2100\n","Training loss  0.608 in Step 2200\n","Training loss  0.539 in Step 2300\n","Training loss  0.598 in Step 2400\n","Training loss  0.542 in Step 2500\n","Training loss  0.487 in Step 2600\n","Training loss  0.516 in Step 2700\n","Training loss  0.558 in Step 2800\n","Training loss  0.775 in Step 2900\n","Training loss  0.397 in Step 3000\n","Training loss  0.264 in Step 3100\n","Training loss  0.552 in Step 3200\n","Training loss  0.616 in Step 3300\n","Training loss  0.534 in Step 3400\n","Training loss  0.521 in Step 3500\n","Training loss  0.605 in Step 3600\n","Training loss  0.582 in Step 3700\n","Training loss  0.653 in Step 3800\n","Training loss  0.491 in Step 3900\n","Training loss  0.486 in Step 4000\n","Training loss  0.472 in Step 4100\n","Training loss  0.596 in Step 4200\n","Training loss  0.440 in Step 4300\n","Training loss  0.396 in Step 4400\n","Training loss  0.535 in Step 4500\n","Training loss  0.575 in Step 4600\n","Training loss  0.451 in Step 4700\n","Training loss  0.584 in Step 4800\n","Training loss  0.642 in Step 4900\n","Training loss  0.397 in Step 5000\n","Training loss  0.348 in Step 5100\n","Training loss  0.429 in Step 5200\n","Training loss  0.438 in Step 5300\n","Training loss  0.423 in Step 5400\n","Training loss  0.474 in Step 5500\n","Training loss  0.539 in Step 5600\n","Training loss  0.598 in Step 5700\n","Training loss  0.490 in Step 5800\n","Training loss  0.589 in Step 5900\n","Training loss  0.564 in Step 6000\n","Training loss  0.506 in Step 6100\n","Training loss  0.751 in Step 6200\n","Training loss  0.531 in Step 6300\n","Training loss  0.623 in Step 6400\n","Training loss  0.648 in Step 6500\n","Training loss  0.549 in Step 6600\n","Training loss  0.460 in Step 6700\n","Training loss  0.429 in Step 6800\n","Training loss  0.411 in Step 6900\n","Training loss  0.529 in Step 7000\n","Training loss  0.463 in Step 7100\n","Training loss  0.545 in Step 7200\n","※※※Training loss  0.530※※※\n","Training timepoint saved\n","Valid loss  0.581 in Step 0\n","Valid loss  0.375 in Step 100\n","Valid loss  0.538 in Step 200\n","Valid loss  0.755 in Step 300\n","Valid loss  0.456 in Step 400\n","Valid loss  0.443 in Step 500\n","Valid loss  0.805 in Step 600\n","Valid loss  0.536 in Step 700\n","Valid loss  0.520 in Step 800\n","Valid loss  0.578 in Step 900\n","Valid loss  0.724 in Step 1000\n","Valid loss  0.428 in Step 1100\n","Valid loss  0.474 in Step 1200\n","Valid loss  0.354 in Step 1300\n","Valid loss  0.233 in Step 1400\n","Valid loss  0.256 in Step 1500\n","Valid loss  0.411 in Step 1600\n","Valid loss  0.533※※※\n","Epoch 29\n","Training loss  0.397 in Step 0\n","Training loss  0.333 in Step 100\n","Training loss  0.682 in Step 200\n","Training loss  0.684 in Step 300\n","Training loss  0.647 in Step 400\n","Training loss  0.711 in Step 500\n","Training loss  0.683 in Step 600\n","Training loss  0.527 in Step 700\n","Training loss  0.475 in Step 800\n","Training loss  0.558 in Step 900\n","Training loss  0.290 in Step 1000\n","Training loss  0.446 in Step 1100\n","Training loss  0.552 in Step 1200\n","Training loss  0.495 in Step 1300\n","Training loss  0.559 in Step 1400\n","Training loss  0.637 in Step 1500\n","Training loss  0.560 in Step 1600\n","Training loss  0.564 in Step 1700\n","Training loss  0.605 in Step 1800\n","Training loss  0.716 in Step 1900\n","Training loss  0.641 in Step 2000\n","Training loss  0.453 in Step 2100\n","Training loss  0.604 in Step 2200\n","Training loss  0.546 in Step 2300\n","Training loss  0.610 in Step 2400\n","Training loss  0.549 in Step 2500\n","Training loss  0.476 in Step 2600\n","Training loss  0.513 in Step 2700\n","Training loss  0.542 in Step 2800\n","Training loss  0.767 in Step 2900\n","Training loss  0.400 in Step 3000\n","Training loss  0.259 in Step 3100\n","Training loss  0.552 in Step 3200\n","Training loss  0.615 in Step 3300\n","Training loss  0.525 in Step 3400\n","Training loss  0.549 in Step 3500\n","Training loss  0.602 in Step 3600\n","Training loss  0.589 in Step 3700\n","Training loss  0.664 in Step 3800\n","Training loss  0.487 in Step 3900\n","Training loss  0.491 in Step 4000\n","Training loss  0.468 in Step 4100\n","Training loss  0.597 in Step 4200\n","Training loss  0.437 in Step 4300\n","Training loss  0.401 in Step 4400\n","Training loss  0.539 in Step 4500\n","Training loss  0.561 in Step 4600\n","Training loss  0.451 in Step 4700\n","Training loss  0.571 in Step 4800\n","Training loss  0.639 in Step 4900\n","Training loss  0.386 in Step 5000\n","Training loss  0.356 in Step 5100\n","Training loss  0.432 in Step 5200\n","Training loss  0.424 in Step 5300\n","Training loss  0.417 in Step 5400\n","Training loss  0.477 in Step 5500\n","Training loss  0.546 in Step 5600\n","Training loss  0.596 in Step 5700\n","Training loss  0.486 in Step 5800\n","Training loss  0.581 in Step 5900\n","Training loss  0.557 in Step 6000\n","Training loss  0.506 in Step 6100\n","Training loss  0.766 in Step 6200\n","Training loss  0.541 in Step 6300\n","Training loss  0.625 in Step 6400\n","Training loss  0.649 in Step 6500\n","Training loss  0.563 in Step 6600\n","Training loss  0.457 in Step 6700\n","Training loss  0.430 in Step 6800\n","Training loss  0.417 in Step 6900\n","Training loss  0.530 in Step 7000\n","Training loss  0.446 in Step 7100\n","Training loss  0.547 in Step 7200\n","※※※Training loss  0.529※※※\n","Training timepoint saved\n","Valid loss  0.585 in Step 0\n","Valid loss  0.376 in Step 100\n","Valid loss  0.532 in Step 200\n","Valid loss  0.769 in Step 300\n","Valid loss  0.448 in Step 400\n","Valid loss  0.452 in Step 500\n","Valid loss  0.813 in Step 600\n","Valid loss  0.552 in Step 700\n","Valid loss  0.510 in Step 800\n","Valid loss  0.574 in Step 900\n","Valid loss  0.719 in Step 1000\n","Valid loss  0.422 in Step 1100\n","Valid loss  0.475 in Step 1200\n","Valid loss  0.355 in Step 1300\n","Valid loss  0.229 in Step 1400\n","Valid loss  0.253 in Step 1500\n","Valid loss  0.405 in Step 1600\n","Valid loss  0.532※※※\n"]}]},{"cell_type":"code","source":["### Save\n","valid_losses.save()\n","train_losses.save()\n","text_hist.save()"],"metadata":{"id":"KSTTwi31xAvh"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"id":"3yaMyIzH12RD","executionInfo":{"status":"ok","timestamp":1675061211822,"user_tz":-480,"elapsed":295,"user":{"displayName":"Frank Learning","userId":"10777113402383858671"}},"outputId":"1426c24a-c60c-48c2-8690-f3a07bb9ba7b"},"source":["plt.plot(train_losses.get(), label='Train')\n","plt.plot(valid_losses.get(), label='Valid')\n","plt.title(\"Learning Curve\")\n","plt.legend();"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["ts"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"_UcLyHcQMGu0","executionInfo":{"status":"ok","timestamp":1675061213564,"user_tz":-480,"elapsed":238,"user":{"displayName":"Frank Learning","userId":"10777113402383858671"}},"outputId":"4b56200c-1dc4-4286-b51a-d19e9c75ddd9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'0130021416'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["We can see that with Sigmoid the model seems to perform slightly worse, but this should not be a big problem. Then the decision might depend more on whether the use of it is common and justifiable. I don't think it's common, and the use of it does not seem to be analytically necessary. "],"metadata":{"id":"E6-6g-sW3zeg"}},{"cell_type":"code","source":["class ResAECluster(ResAE): \n","    def __init__(self, input_dim=INPUT_DIM, inter_dim1=INTER_DIM_1, inter_dim2=INTER_DIM_2, inter_dim3=INTER_DIM_3, latent_dim=LATENT_DIM, output_dim=OUTPUT_DIM): \n","        super().__init__(input_dim, inter_dim1, inter_dim2, inter_dim3, latent_dim, output_dim)\n","\n","    def forward(self, x):\n","        org_size = x.size()\n","        batch = org_size[0]\n","        x = x.view(batch, -1)\n","\n","        h = self.encoder(x)\n","        # mu, logvar = h.chunk(2, dim=1)\n","        # z = self.reparameterise(mu, logvar)\n","\n","        return h"],"metadata":{"id":"tNJ0DAlaWrEE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seq = \"_01_05\"\n","tags = pd.read_csv(tags_name + seq + \".csv\")\n","gsds = GroundedSoundDataset(tags, test_name + seq + \".npy\")\n","eval_loader = DataLoader(gsds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"],"metadata":{"id":"BFrvzaGC45Gi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["last_model_name"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"QARr-C_qpBLb","executionInfo":{"status":"ok","timestamp":1675061249095,"user_tz":-480,"elapsed":23,"user":{"displayName":"Frank Learning","userId":"10777113402383858671"}},"outputId":"acd5e8ab-1ef6-4968-c205-b437ecfd7b8e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'model_english_0130021416_29_full'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["# model_name = last_model_name\n","model_name = \"model_english_0130021416_13_full\"\n","model_path = save_dir + model_name + \".pt\"\n","state = torch.load(model_path)\n","model = ResAECluster()\n","model.load_state_dict(state)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","hiddens = None\n","tags = None\n","model.eval()\n","with torch.no_grad():\n","    for idx, (s, e, t) in enumerate(eval_loader):\n","        s = s.to(device)\n","        hidden = model(s)\n","        hidden = hidden.cpu().data.numpy()\n","\n","        if hiddens is not None: \n","            hiddens = np.concatenate((hiddens, hidden), axis=0)\n","            tags = np.concatenate((tags, t), axis=0)\n","        else: \n","            hiddens = hidden\n","            tags = t\n","num_phones = np.unique(tags).shape[0]\n","kmeansmodel = KMeans(n_clusters=num_phones) # , random_state=0\n","clusters = kmeansmodel.fit_predict(hiddens)\n","np.save(save_dir + model_name + seq + \"_hiddenclusters.npy\", clusters)\n","np.save(save_dir + model_name + seq + \"_hiddenrepresentation.npy\", hiddens)\n"],"metadata":{"id":"KXLqyT3PYFkR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["h, c, v = homogeneity_completeness_v_measure(tags, clusters)"],"metadata":{"id":"BzTuc2Mz6niT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(seq, h, c, v) # trained on sampled data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ogsEovzEbpc","executionInfo":{"status":"ok","timestamp":1675766436803,"user_tz":-480,"elapsed":7,"user":{"displayName":"Frank Learning","userId":"10777113402383858671"}},"outputId":"3cd43d32-f30c-4fb3-f5eb-945d6dd0ecc6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["_01_05 0.30813685860010276 0.2726217590636009 0.2892933823757265\n"]}]},{"cell_type":"code","source":["# _17_24 0.3429902101084872 0.329164358854651 0.33593508938856537   # 256+8\n","# _17_24 0.3071758873778334 0.2958512436337788 0.3014072290332542   # 128+4\n","# _17_24 0.3048181747064378 0.303971996633573 0.3043944976042278    # 128+2\n","# _17_24 0.3109960687106377 0.3020004935745723 0.3064322772063619   # 256+2, 2res\n","# _17_24 0.27632046463064963 0.29796767719078493 0.28673608598337974    # 256+64+2, 2res, new model\n","# _17_24 0.29619001674434664 0.30940705212658304 0.30265430485339223    # 256+64+4, 0res\n","# _17_24 0.3394207670351701 0.3356821861468344 0.33754112484613674      # 256+64+4, 2res\n","# _17_24 0.3246121630042821 0.3173438869288583 0.32093687897447765  # 256+3, 2res, not very bad. So we may try this. This is error, decoder only having 1 res\n","# _17_24 0.3227539602867097 0.32256957773330264 0.3226617426690128  # 256+3, 2res\n","# _17_24 0.3403517130774138 0.33762198107176034 0.33898135170147237 # 256+3, 1res\n","# _17_24 0.3202704367215642 0.31097127191607643 0.3155523587925454  # 256+3, 0res\n","\n","\n","# _01_05 0.30784101366300043 0.2717512535534188 0.28867252408265254"],"metadata":{"id":"FMFbNimpx-iJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["总的来说分成四个，神经网层来进行降维处理，得到的损失比较大，但是 hcv值倒是接近不过，如果能尽量的接近原作的模型结构，我们就不去动它了，所以可能目前来看最好的是保留两个降为层加上两个残差层，最后从256降到2，也许是最好的结果当然降到4也是可以的，都是比较低的维度，不过如果我们想要直接能够，在，可视的空间中画出这些点来，2或者3可能会比四更好一些。 "],"metadata":{"id":"_gYDGP0Cdf1o"}},{"cell_type":"markdown","source":["从使用不同数量残插块儿的实验结果来看，是由一个残渣块，应该是最好的解决方式，使用零个或两个第三个都可能是都会使hcv值相对降低。由此来看在选择，隐性层纬度为三的情况下，我们应该选择适用一个参差款。"],"metadata":{"id":"nTTdgp_HokAn"}},{"cell_type":"markdown","source":["### Conclusion\n","Adding new data slightly improves the performance of the model in HCV score, in addition, shuffling the training data largely lowers the HGV score perhaps we should discuss this phenomenon and justify use no shuffling during training. Perhaps this is because of some sort of phonotactics or naturalness of sound streams. "],"metadata":{"id":"BxdQ9f85WY1K"}},{"cell_type":"markdown","source":["Good news is that for the English model it performs similar well. "],"metadata":{"id":"kjoJ2fFKpmVC"}},{"cell_type":"code","source":[],"metadata":{"id":"ZjPWgjpid7PR"},"execution_count":null,"outputs":[]}]}